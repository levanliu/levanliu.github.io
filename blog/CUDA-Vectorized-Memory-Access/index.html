<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://leimao.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>CUDA Vectorized Memory Access - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="Accelerating CUDA Data Transfer"><meta property="og:type" content="blog"><meta property="og:title" content="CUDA Vectorized Memory Access"><meta property="og:url" content="https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="Accelerating CUDA Data Transfer"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://leimao.github.io/images/favicon/android-chrome-512x512.png"><meta property="article:published_time" content="2024-01-14T08:00:00.000Z"><meta property="article:modified_time" content="2024-01-14T08:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="NVIDIA"><meta property="article:tag" content="GPU"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://leimao.github.io/images/favicon/android-chrome-512x512.png"><meta property="twitter:creator" content="@matchaleimao"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukeleimao"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/"},"headline":"CUDA Vectorized Memory Access","image":["https://leimao.github.io/images/favicon/android-chrome-512x512.png"],"datePublished":"2024-01-14T08:00:00.000Z","dateModified":"2024-01-14T08:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://leimao.github.io/images/favicon/android-chrome-512x512.png"}},"description":"Accelerating CUDA Data Transfer"}</script><link rel="canonical" href="https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/leimao/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:leimaogithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">CUDA Vectorized Memory Access</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2024-01-14T08:00:00.000Z" title="2024-01-14T08:00:00.000Z">01-14-2024</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2024-01-14T08:00:00.000Z" title="2024-01-14T08:00:00.000Z">01-14-2024</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 30 minutes read (About 4505 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Reading and writing data from and to the DRAM is one of the fundamental operations in CUDA programming. The effective memory bandwidth of the CUDA device is one of the most critical factors that affect the performance of a CUDA function, especially when the CUDA function is memory-bound.</p>
<p>In this blog post, we will show how to improve the effective memory bandwidth of a CUDA function by using vectorized memory access.</p>
<h2 id="CUDA-Vectorized-Memory-Access"><a href="#CUDA-Vectorized-Memory-Access" class="headerlink" title="CUDA Vectorized Memory Access"></a>CUDA Vectorized Memory Access</h2><p>In the following example, we will implement a naive custom device memcpy function and show how to improve its effective memory bandwidth via 8-byte or 16-byte per thread vectorized memory transactions for contiguous data of different data types. The consequence of using 8-byte or 16-byte per thread vectorized memory transactions is that the number of memory transactions required for data copy gets reduced, which improves the effective memory bandwidth in almost all the use cases.</p>
<figure class="highlight cpp"><figcaption><span>memcpy.cu</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;type_traits&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">check</span><span class="params">(cudaError_t err, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; func &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_LAST_CUDA_ERROR() check_last(__FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">check_last</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file, <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaError_t <span class="type">const</span> err&#123;<span class="built_in">cudaGetLastError</span>()&#125;;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::string <span class="title">std_string_centered</span><span class="params">(std::string <span class="type">const</span>&amp; s, <span class="type">size_t</span> width,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">char</span> pad = <span class="string">&#x27; &#x27;</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> l&#123;s.<span class="built_in">length</span>()&#125;;</span><br><span class="line">    <span class="comment">// Throw an exception if width is too small.</span></span><br><span class="line">    <span class="keyword">if</span> (width &lt; l)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">throw</span> std::<span class="built_in">runtime_error</span>(<span class="string">&quot;Width is too small.&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> left_pad&#123;(width - l) / <span class="number">2</span>&#125;;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> right_pad&#123;width - l - left_pad&#125;;</span><br><span class="line">    std::string <span class="type">const</span> s_centered&#123;std::<span class="built_in">string</span>(left_pad, pad) + s +</span><br><span class="line">                                 std::<span class="built_in">string</span>(right_pad, pad)&#125;;</span><br><span class="line">    <span class="keyword">return</span> s_centered;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_performance</span><span class="params">(std::function&lt;T(cudaStream_t)&gt; <span class="type">const</span>&amp; bound_function,</span></span></span><br><span class="line"><span class="params"><span class="function">                          cudaStream_t stream, <span class="type">unsigned</span> <span class="type">int</span> num_repeats = <span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">unsigned</span> <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> time;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;stop));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0U</span>&#125;; i &lt; num_warmups; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(start, stream));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0U</span>&#125;; i &lt; num_repeats; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(stop, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(stop));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency&#123;time / num_repeats&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">custom_device_memcpy</span><span class="params">(T* __restrict__ output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                     T <span class="type">const</span>* __restrict__ input, <span class="type">size_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> idx&#123;blockDim.x * blockIdx.x + threadIdx.x&#125;;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> stride&#123;blockDim.x * gridDim.x&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;idx&#125;; i &lt; n; i += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        output[i] = input[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_custom_device_memcpy</span><span class="params">(T* output, T <span class="type">const</span>* input, <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                                 cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    dim3 <span class="type">const</span> threads_per_block&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    dim3 <span class="type">const</span> blocks_per_grid&#123;<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;(std::<span class="built_in">min</span>(</span><br><span class="line">        (n + threads_per_block.x - <span class="number">1U</span>) / threads_per_block.x,</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(std::numeric_limits&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;::<span class="built_in">max</span>())))&#125;;</span><br><span class="line">    custom_device_memcpy&lt;&lt;&lt;blocks_per_grid, threads_per_block, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">        output, input, n);</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="type">unsigned</span> <span class="type">int</span> BLOCK_DIM_X&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">custom_device_memcpy_shared_memory</span><span class="params">(T* __restrict__ output,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                   T <span class="type">const</span>* __restrict__ input,</span></span></span><br><span class="line"><span class="params"><span class="function">                                                   <span class="type">size_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Using shared memory as intermediate buffer.</span></span><br><span class="line">    __shared__ T shared_memory[BLOCK_DIM_X];</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> idx&#123;blockDim.x * blockIdx.x + threadIdx.x&#125;;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> stride&#123;blockDim.x * gridDim.x&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;idx&#125;; i &lt; n; i += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        shared_memory[threadIdx.x] = input[i];</span><br><span class="line">        <span class="comment">// Synchronization is not necessary in this case.</span></span><br><span class="line">        <span class="comment">// __syncthreads();</span></span><br><span class="line">        output[i] = shared_memory[threadIdx.x];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_custom_device_memcpy_shared_memory</span><span class="params">(T* output, T <span class="type">const</span>* input,</span></span></span><br><span class="line"><span class="params"><span class="function">                                               <span class="type">size_t</span> n, cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> dim3 threads_per_block&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    dim3 <span class="type">const</span> blocks_per_grid&#123;<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;(std::<span class="built_in">min</span>(</span><br><span class="line">        (n + threads_per_block.x - <span class="number">1U</span>) / threads_per_block.x,</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(std::numeric_limits&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;::<span class="built_in">max</span>())))&#125;;</span><br><span class="line">    custom_device_memcpy_shared_memory&lt;T, threads_per_block.x&gt;</span><br><span class="line">        &lt;&lt;&lt;blocks_per_grid, threads_per_block, <span class="number">0</span>, stream&gt;&gt;&gt;(output, input, n);</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// One thread copies sizeof(R) bytes of data.</span></span><br><span class="line"><span class="comment">// One warp copies 32 x sizeof(R) bytes of data via one of few memory</span></span><br><span class="line"><span class="comment">// transactions.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R = <span class="type">uint64_t</span>&gt;</span><br><span class="line">__global__ <span class="type">void</span> <span class="built_in">custom_device_memcpy_optimized</span>(T* __restrict__ output,</span><br><span class="line">                                               T <span class="type">const</span>* __restrict__ input,</span><br><span class="line">                                               <span class="type">size_t</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> idx&#123;blockDim.x * blockIdx.x + threadIdx.x&#125;;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> stride&#123;blockDim.x * gridDim.x&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;idx&#125;; i * <span class="built_in">sizeof</span>(R) / <span class="built_in">sizeof</span>(T) &lt; n; i += stride)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> ((i + <span class="number">1U</span>) * <span class="built_in">sizeof</span>(R) / <span class="built_in">sizeof</span>(T) &lt; n)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">reinterpret_cast</span>&lt;R*&gt;(output)[i] =</span><br><span class="line">                <span class="built_in">reinterpret_cast</span>&lt;R <span class="type">const</span>*&gt;(input)[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Remaining units to copy.</span></span><br><span class="line">            <span class="type">size_t</span> <span class="type">const</span> start_index&#123;i * <span class="built_in">sizeof</span>(R) / <span class="built_in">sizeof</span>(T)&#125;;</span><br><span class="line">            <span class="type">size_t</span> <span class="type">const</span> remaining_units_to_copy&#123;(n - start_index)&#125;;</span><br><span class="line">            <span class="keyword">for</span> (<span class="type">size_t</span> j&#123;<span class="number">0</span>&#125;; j &lt; remaining_units_to_copy; ++j)</span><br><span class="line">            &#123;</span><br><span class="line">                output[start_index + j] = input[start_index + j];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">typename</span> R = <span class="type">uint64_t</span>&gt;</span><br><span class="line"><span class="type">void</span> <span class="built_in">launch_custom_device_memcpy_optimized</span>(T* output, T <span class="type">const</span>* input, <span class="type">size_t</span> n,</span><br><span class="line">                                           cudaStream_t stream)</span><br><span class="line">&#123;</span><br><span class="line">    dim3 <span class="type">const</span> threads_per_block&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> num_units_to_copy_round_up&#123;(n * <span class="built_in">sizeof</span>(T) + <span class="built_in">sizeof</span>(R) - <span class="number">1U</span>) /</span><br><span class="line">                                            <span class="built_in">sizeof</span>(R)&#125;;</span><br><span class="line">    dim3 <span class="type">const</span> blocks_per_grid&#123;<span class="built_in">static_cast</span>&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;(std::<span class="built_in">min</span>(</span><br><span class="line">        (num_units_to_copy_round_up + threads_per_block.x - <span class="number">1U</span>) /</span><br><span class="line">            threads_per_block.x,</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(std::numeric_limits&lt;<span class="type">unsigned</span> <span class="type">int</span>&gt;::<span class="built_in">max</span>())))&#125;;</span><br><span class="line">    custom_device_memcpy_optimized&lt;&lt;&lt;blocks_per_grid, threads_per_block, <span class="number">0</span>,</span><br><span class="line">                                     stream&gt;&gt;&gt;(output, input, n);</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_official_device_memcpy</span><span class="params">(T* output, T <span class="type">const</span>* input, <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                                   cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(output, input, n * <span class="built_in">sizeof</span>(T),</span><br><span class="line">                                     cudaMemcpyDeviceToDevice, stream));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Initialize the buffer so that the unit of the data is the index of the data.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, std::<span class="type">enable_if_t</span>&lt;std::is_integral&lt;T&gt;::value, <span class="type">bool</span>&gt; = <span class="literal">true</span>&gt;</span><br><span class="line"><span class="type">void</span> <span class="built_in">initialize_buffer</span>(T* buffer, <span class="type">size_t</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        buffer[i] = <span class="built_in">static_cast</span>&lt;T&gt;(</span><br><span class="line">            i % <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(std::numeric_limits&lt;T&gt;::<span class="built_in">max</span>()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, std::<span class="type">enable_if_t</span>&lt;std::is_integral&lt;T&gt;::value, <span class="type">bool</span>&gt; = <span class="literal">true</span>&gt;</span><br><span class="line"><span class="type">void</span> <span class="built_in">verify_buffer</span>(T* buffer, <span class="type">size_t</span> n)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (buffer[i] != <span class="built_in">static_cast</span>&lt;T&gt;(i % <span class="built_in">static_cast</span>&lt;<span class="type">size_t</span>&gt;(</span><br><span class="line">                                                std::numeric_limits&lt;T&gt;::<span class="built_in">max</span>())))</span><br><span class="line">        &#123;</span><br><span class="line">            std::cerr &lt;&lt; <span class="string">&quot;Verification failed at index: &quot;</span> &lt;&lt; i &lt;&lt; std::endl;</span><br><span class="line">            std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Measure custom device memcpy performance given the number of units to copy,</span></span><br><span class="line"><span class="comment">// the device memcpy function to use, and the number of repeats and warmups.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_custom_device_memcpy_performance</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">    std::function&lt;<span class="type">void</span>(T*, T <span class="type">const</span>*, <span class="type">size_t</span>, cudaStream_t)&gt; <span class="type">const</span>&amp;</span></span></span><br><span class="line"><span class="params"><span class="function">        device_memcpy_function,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span> num_repeats = <span class="number">100</span>, <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamCreateWithFlags</span>(&amp;stream, cudaStreamNonBlocking));</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;T&gt; <span class="title">input</span><span class="params">(n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;T&gt; <span class="title">output</span><span class="params">(n, <span class="keyword">static_cast</span>&lt;T&gt;(<span class="number">0</span>))</span></span>;</span><br><span class="line">    <span class="built_in">initialize_buffer</span>(input.<span class="built_in">data</span>(), n);</span><br><span class="line"></span><br><span class="line">    T* d_input;</span><br><span class="line">    T* d_output;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;d_input, n * <span class="built_in">sizeof</span>(T)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;d_output, n * <span class="built_in">sizeof</span>(T)));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(d_input, input.<span class="built_in">data</span>(), n * <span class="built_in">sizeof</span>(T),</span><br><span class="line">                                     cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(d_output, output.<span class="built_in">data</span>(), n * <span class="built_in">sizeof</span>(T),</span><br><span class="line">                                     cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">// Run device memcpy once to check correcness.</span></span><br><span class="line">    <span class="built_in">device_memcpy_function</span>(d_output, d_input, n, stream);</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(output.<span class="built_in">data</span>(), d_output, n * <span class="built_in">sizeof</span>(T),</span><br><span class="line">                                     cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verify the correctness of the device memcpy.</span></span><br><span class="line">    <span class="built_in">verify_buffer</span>(output.<span class="built_in">data</span>(), n);</span><br><span class="line"></span><br><span class="line">    <span class="type">size_t</span> <span class="type">const</span> num_bytes&#123;<span class="function">n * <span class="title">sizeof</span><span class="params">(T)</span>&#125;</span>;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> num_giga_bytes&#123;<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(num_bytes) / (<span class="number">1</span> &lt;&lt; <span class="number">30</span>)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;<span class="type">void</span>(cudaStream_t)&gt; function&#123;std::<span class="built_in">bind</span>(</span><br><span class="line">        device_memcpy_function, d_output, d_input, n, std::placeholders::_1)&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency&#123;</span><br><span class="line">        <span class="built_in">measure_performance</span>(function, stream, num_repeats, num_warmups)&#125;;</span><br><span class="line">    std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; <span class="string">&quot;Latency: &quot;</span> &lt;&lt; latency</span><br><span class="line">              &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Effective Bandwitdh: &quot;</span></span><br><span class="line">              &lt;&lt; <span class="number">2.f</span> * num_giga_bytes / (latency / <span class="number">1000</span>) &lt;&lt; <span class="string">&quot; GB/s&quot;</span></span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_input));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_output));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamDestroy</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Query deive name and peak memory bandwidth.</span></span><br><span class="line">    <span class="type">int</span> device_id&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="built_in">cudaGetDevice</span>(&amp;device_id);</span><br><span class="line">    cudaDeviceProp device_prop;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;device_prop, device_id);</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> peak_bandwidth&#123;</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(<span class="number">2.0</span> * device_prop.memoryClockRate *</span><br><span class="line">                           (device_prop.memoryBusWidth / <span class="number">8</span>) / <span class="number">1.0e6</span>)&#125;;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Percentage of Peak Bandwitdh: &quot;</span></span><br><span class="line">              &lt;&lt; <span class="number">2.f</span> * num_giga_bytes / (latency / <span class="number">1000</span>) / peak_bandwidth * <span class="number">100</span></span><br><span class="line">              &lt;&lt; <span class="string">&quot;%&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> num_repeats&#123;<span class="number">10U</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> num_warmups&#123;<span class="number">10U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> tensor_size_small&#123;<span class="number">1U</span> * <span class="number">64U</span> * <span class="number">64U</span> * <span class="number">64U</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> tensor_size_medium&#123;<span class="number">1U</span> * <span class="number">128U</span> * <span class="number">128U</span> * <span class="number">128U</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> tensor_size_large&#123;<span class="number">1U</span> * <span class="number">512U</span> * <span class="number">512U</span> * <span class="number">512U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">size_t</span> string_width&#123;<span class="number">50U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;~&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;NVIDIA GPU Device Info&quot;</span>, string_width,</span><br><span class="line">                                     <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;~&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Query deive name and peak memory bandwidth.</span></span><br><span class="line">    <span class="type">int</span> device_id&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="built_in">cudaGetDevice</span>(&amp;device_id);</span><br><span class="line">    cudaDeviceProp device_prop;</span><br><span class="line">    <span class="built_in">cudaGetDeviceProperties</span>(&amp;device_prop, device_id);</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Device Name: &quot;</span> &lt;&lt; device_prop.name &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> memory_size&#123;<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(device_prop.totalGlobalMem) /</span><br><span class="line">                            (<span class="number">1</span> &lt;&lt; <span class="number">30</span>)&#125;;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Memory Size: &quot;</span> &lt;&lt; memory_size &lt;&lt; <span class="string">&quot; GB&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> peak_bandwidth&#123;</span><br><span class="line">        <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(<span class="number">2.0f</span> * device_prop.memoryClockRate *</span><br><span class="line">                           (device_prop.memoryBusWidth / <span class="number">8</span>) / <span class="number">1.0e6</span>)&#125;;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Peak Bandwitdh: &quot;</span> &lt;&lt; peak_bandwidth &lt;&lt; <span class="string">&quot; GB/s&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Measure CUDA official memcpy performance for different tensor sizes.</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;CUDA Official Memcpy&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_official_device_memcpy&lt;<span class="type">int8_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_official_device_memcpy&lt;<span class="type">int16_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_official_device_memcpy&lt;<span class="type">int32_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_official_device_memcpy&lt;<span class="type">int64_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Measure the latency and bandwidth of custom device memcpy for different</span></span><br><span class="line">    <span class="comment">// tensor sizes.</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Custom Device Memcpy&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy&lt;<span class="type">int8_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy&lt;<span class="type">int16_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy&lt;<span class="type">int32_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy&lt;<span class="type">int64_t</span>&gt;, num_repeats,</span><br><span class="line">            num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Conclusions:</span></span><br><span class="line">    <span class="comment">// 1. The more units of data we copy, the higher the bandwidth.</span></span><br><span class="line">    <span class="comment">// 2. The larger the unit of the data, the higher the bandwidth.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Check if shared memory can improve the latency of custom device memcpy.</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Custom Device Memcpy with Shared Memory&quot;</span>,</span><br><span class="line">                                     string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_shared_memory&lt;<span class="type">int8_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_shared_memory&lt;<span class="type">int16_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_shared_memory&lt;<span class="type">int32_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_shared_memory&lt;<span class="type">int64_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Conclusions:</span></span><br><span class="line">    <span class="comment">// 1. The effect of using shared memory for improving the latency of custom</span></span><br><span class="line">    <span class="comment">// device memcpy is not obvious.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Improve the latency of custom device memcpy when the unit of the data is</span></span><br><span class="line">    <span class="comment">// small.</span></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(</span><br><span class="line">                     <span class="string">&quot;Custom Device Memcpy 4-Byte Copy Per Thread&quot;</span>,</span><br><span class="line">                     string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int8_t</span>, <span class="type">uint32_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int16_t</span>, <span class="type">uint32_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int32_t</span>, <span class="type">uint32_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int64_t</span>, <span class="type">uint32_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(</span><br><span class="line">                     <span class="string">&quot;Custom Device Memcpy 8-Byte Copy Per Thread&quot;</span>,</span><br><span class="line">                     string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int8_t</span>, <span class="type">uint64_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int16_t</span>, <span class="type">uint64_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int32_t</span>, <span class="type">uint64_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size,</span><br><span class="line">            launch_custom_device_memcpy_optimized&lt;<span class="type">int64_t</span>, <span class="type">uint64_t</span>&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(</span><br><span class="line">                     <span class="string">&quot;Custom Device Memcpy 16-Byte Copy Per Thread&quot;</span>,</span><br><span class="line">                     string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">              &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;*&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> tensor_size :</span><br><span class="line">         &#123;tensor_size_small, tensor_size_medium, tensor_size_large&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        std::string <span class="type">const</span> tensor_size_string&#123;std::<span class="built_in">string</span>(<span class="string">&quot;Tensor Size: &quot;</span>) +</span><br><span class="line">                                             std::<span class="built_in">to_string</span>(tensor_size) +</span><br><span class="line">                                             std::<span class="built_in">string</span>(<span class="string">&quot; Units&quot;</span>)&#125;;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(tensor_size_string, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;=&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 1 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int8_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_optimized&lt;<span class="type">int8_t</span>, uint4&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 2 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int16_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_optimized&lt;<span class="type">int16_t</span>, uint4&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 4 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int32_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_optimized&lt;<span class="type">int32_t</span>, uint4&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;Unit Size: 8 Byte&quot;</span>, string_width, <span class="string">&#x27; &#x27;</span>)</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cout &lt;&lt; <span class="built_in">std_string_centered</span>(<span class="string">&quot;&quot;</span>, string_width, <span class="string">&#x27;-&#x27;</span>) &lt;&lt; std::endl;</span><br><span class="line">        <span class="built_in">measure_custom_device_memcpy_performance</span>&lt;<span class="type">int64_t</span>&gt;(</span><br><span class="line">            tensor_size, launch_custom_device_memcpy_optimized&lt;<span class="type">int64_t</span>, uint4&gt;,</span><br><span class="line">            num_repeats, num_warmups);</span><br><span class="line">    &#125;</span><br><span class="line">    std::cout &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Conclusions:</span></span><br><span class="line">    <span class="comment">// 1. Copying data in units of 8 bytes or 16 bytes can improve the latency</span></span><br><span class="line">    <span class="comment">// of custom device memcpy.</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The CUDA program was compiled and profiled on an NVIDIA RTX 3090 GPU with CUDA 12.0.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc memcpy.cu -o memcpy -std=c++14</span><br><span class="line">$ ./memcpy</span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">              NVIDIA GPU Device Info</span><br><span class="line">~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~</span><br><span class="line">Device Name: NVIDIA GeForce RTX 3090</span><br><span class="line">Memory Size: 23.6694 GB</span><br><span class="line">Peak Bandwitdh: 936.096 GB/s</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">               CUDA Official Memcpy</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 217.362 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 23.220%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 414.641 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 44.295%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 706.425 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 75.465%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 1030.999 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 110.138%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 1059.638 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 113.198%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.011 ms</span><br><span class="line">Effective Bandwitdh: 719.754 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 76.889%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.023 ms</span><br><span class="line">Effective Bandwitdh: 675.261 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.136%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.043 ms</span><br><span class="line">Effective Bandwitdh: 719.330 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 76.844%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.321 ms</span><br><span class="line">Effective Bandwitdh: 778.091 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.121%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.640 ms</span><br><span class="line">Effective Bandwitdh: 781.539 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.489%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.275 ms</span><br><span class="line">Effective Bandwitdh: 784.214 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.775%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.560 ms</span><br><span class="line">Effective Bandwitdh: 781.282 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.462%</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">               Custom Device Memcpy</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 183.399 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 19.592%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 354.443 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 37.864%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 681.196 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.770%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 1192.093 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 127.347%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.010 ms</span><br><span class="line">Effective Bandwitdh: 378.747 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 40.460%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.018 ms</span><br><span class="line">Effective Bandwitdh: 445.593 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 47.601%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.024 ms</span><br><span class="line">Effective Bandwitdh: 660.732 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 70.584%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.042 ms</span><br><span class="line">Effective Bandwitdh: 737.140 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 78.746%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.972 ms</span><br><span class="line">Effective Bandwitdh: 257.207 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 27.477%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.076 ms</span><br><span class="line">Effective Bandwitdh: 464.543 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 49.626%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.369 ms</span><br><span class="line">Effective Bandwitdh: 730.586 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 78.046%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.536 ms</span><br><span class="line">Effective Bandwitdh: 788.727 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 84.257%</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">     Custom Device Memcpy with Shared Memory</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 175.995 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 18.801%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 328.853 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 35.130%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 653.481 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 69.809%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 1128.192 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 120.521%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.011 ms</span><br><span class="line">Effective Bandwitdh: 353.213 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 37.733%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.018 ms</span><br><span class="line">Effective Bandwitdh: 433.488 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 46.308%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.024 ms</span><br><span class="line">Effective Bandwitdh: 650.261 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 69.465%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.042 ms</span><br><span class="line">Effective Bandwitdh: 737.864 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 78.824%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.011 ms</span><br><span class="line">Effective Bandwitdh: 247.181 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 26.406%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.113 ms</span><br><span class="line">Effective Bandwitdh: 449.172 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 47.984%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.391 ms</span><br><span class="line">Effective Bandwitdh: 718.748 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 76.781%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.546 ms</span><br><span class="line">Effective Bandwitdh: 785.429 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.905%</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">   Custom Device Memcpy 4-Byte Copy Per Thread</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 238.419 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 25.469%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 437.842 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 46.773%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 684.251 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 73.096%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 1003.868 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 107.240%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 968.812 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 103.495%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.012 ms</span><br><span class="line">Effective Bandwitdh: 675.168 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.126%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.024 ms</span><br><span class="line">Effective Bandwitdh: 660.196 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 70.527%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.045 ms</span><br><span class="line">Effective Bandwitdh: 690.443 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 73.758%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.366 ms</span><br><span class="line">Effective Bandwitdh: 682.529 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.912%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.722 ms</span><br><span class="line">Effective Bandwitdh: 692.125 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 73.937%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.422 ms</span><br><span class="line">Effective Bandwitdh: 703.431 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 75.145%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.824 ms</span><br><span class="line">Effective Bandwitdh: 708.144 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 75.649%</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">   Custom Device Memcpy 8-Byte Copy Per Thread</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 238.792 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 25.509%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 434.723 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 46.440%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 681.196 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.770%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 1030.999 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 110.138%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.004 ms</span><br><span class="line">Effective Bandwitdh: 978.128 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 104.490%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.012 ms</span><br><span class="line">Effective Bandwitdh: 677.416 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 72.366%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.022 ms</span><br><span class="line">Effective Bandwitdh: 696.748 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 74.431%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.042 ms</span><br><span class="line">Effective Bandwitdh: 738.924 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 78.937%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.320 ms</span><br><span class="line">Effective Bandwitdh: 781.750 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.512%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.636 ms</span><br><span class="line">Effective Bandwitdh: 786.536 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 84.023%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.265 ms</span><br><span class="line">Effective Bandwitdh: 790.547 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 84.451%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.530 ms</span><br><span class="line">Effective Bandwitdh: 790.419 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 84.438%</span><br><span class="line"></span><br><span class="line">**************************************************</span><br><span class="line">   Custom Device Memcpy 16-Byte Copy Per Thread</span><br><span class="line">**************************************************</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 262144 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 216.744 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 23.154%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 414.641 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 44.295%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.002 ms</span><br><span class="line">Effective Bandwitdh: 829.282 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 88.589%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 1192.093 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 127.347%</span><br><span class="line">==================================================</span><br><span class="line">            Tensor Size: 2097152 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.003 ms</span><br><span class="line">Effective Bandwitdh: 1128.192 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 120.521%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.010 ms</span><br><span class="line">Effective Bandwitdh: 755.386 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 80.695%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.023 ms</span><br><span class="line">Effective Bandwitdh: 687.333 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 73.425%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.043 ms</span><br><span class="line">Effective Bandwitdh: 728.343 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 77.806%</span><br><span class="line">==================================================</span><br><span class="line">           Tensor Size: 134217728 Units</span><br><span class="line">==================================================</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 1 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.321 ms</span><br><span class="line">Effective Bandwitdh: 779.006 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.219%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 2 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 0.639 ms</span><br><span class="line">Effective Bandwitdh: 782.639 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.607%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 4 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 1.280 ms</span><br><span class="line">Effective Bandwitdh: 781.520 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.487%</span><br><span class="line">--------------------------------------------------</span><br><span class="line">                Unit Size: 8 Byte</span><br><span class="line">--------------------------------------------------</span><br><span class="line">Latency: 2.552 ms</span><br><span class="line">Effective Bandwitdh: 783.602 GB/s</span><br><span class="line">Percentage of Peak Bandwitdh: 83.710%</span><br></pre></td></tr></table></figure>

<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>We could see from the results that:</p>
<ol>
<li>The more units of data we copy, the higher the effective memory bandwidth.</li>
<li>The larger the unit of the data, the higher the effective memory bandwidth.</li>
<li>Copying data in vectorized units of 8 bytes or 16 bytes can improve the effective memory bandwidth of custom device memcpy in most cases, especially when the unit of the data is small.</li>
<li>The effect of using shared memory for improving the effective memory bandwidth of custom device memcpy is not obvious.</li>
</ol>
<p>Note that even though we could just use the CUDA official memcpy for this use case, it is still good to know how to write and improve a custom device memcpy function, because in more practical CUDA applications, the data to copy may not be contiguous in memory, and we may need to copy data from multiple sources to multiple destinations.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/cuda-pro-tip-increase-performance-with-vectorized-memory-access/">CUDA Pro Tip: Increase Performance with Vectorized Memory Access</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>CUDA Vectorized Memory Access</p><p><a href="https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/">https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>01-14-2024</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>01-14-2024</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/CUDA/">CUDA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/NVIDIA/">NVIDIA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/GPU/">GPU </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/leimao" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/blog/Garmin-Forerunner-265S/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Garmin Forerunner 265S</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/life/Dumbarton-Pier/"><span class="level-item">Dumbarton Pier 徒步</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://leimao.github.io/blog/CUDA-Vectorized-Memory-Access/';
            this.page.identifier = '/blog/CUDA-Vectorized-Memory-Access/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'leimao-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/leimao" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/leimao" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchaleimao"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukeleimao@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#CUDA-Vectorized-Memory-Access"><span class="level-left"><span class="level-item">2</span><span class="level-item">CUDA Vectorized Memory Access</span></span></a></li><li><a class="level is-mobile" href="#Conclusions"><span class="level-left"><span class="level-item">3</span><span class="level-item">Conclusions</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">4</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=leimaogithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>