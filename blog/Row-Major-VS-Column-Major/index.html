<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://leimao.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>Row-Major VS Column-Major - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="Ways of Packing Matrix in Memory and Its Consequence for Matrix Multiplication"><meta property="og:type" content="blog"><meta property="og:title" content="Row-Major VS Column-Major"><meta property="og:url" content="https://leimao.github.io/blog/Row-Major-VS-Column-Major/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="Ways of Packing Matrix in Memory and Its Consequence for Matrix Multiplication"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://leimao.github.io/images/blog/2023-05-12-Row-Major-VS-Column-Major/Row_and_column_major_order.svg"><meta property="article:published_time" content="2023-05-12T07:00:00.000Z"><meta property="article:modified_time" content="2023-05-12T07:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="CPP"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="Computer Architecture"><meta property="article:tag" content="Memory"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://leimao.github.io/images/blog/2023-05-12-Row-Major-VS-Column-Major/Row_and_column_major_order.svg"><meta property="twitter:creator" content="@matchaleimao"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukeleimao"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://leimao.github.io/blog/Row-Major-VS-Column-Major/"},"headline":"Row-Major VS Column-Major","image":[],"datePublished":"2023-05-12T07:00:00.000Z","dateModified":"2023-05-12T07:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://leimao.github.io/images/favicon/android-chrome-512x512.png"}},"description":"Ways of Packing Matrix in Memory and Its Consequence for Matrix Multiplication"}</script><link rel="canonical" href="https://leimao.github.io/blog/Row-Major-VS-Column-Major/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/leimao/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:leimaogithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">Row-Major VS Column-Major</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-05-12T07:00:00.000Z" title="2023-05-12T07:00:00.000Z">05-12-2023</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-05-12T07:00:00.000Z" title="2023-05-12T07:00:00.000Z">05-12-2023</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 28 minutes read (About 4153 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In computing, row-major order and column-major order are two ways for storing multidimensional arrays in linear storage such as random access memory. The following figure demonstrated the row-major order and the column-major for a 2D matrix.</p>
<img src="/images/blog/2023-05-12-Row-Major-VS-Column-Major/Row_and_column_major_order.svg" class="box px-0 py-0 ml-auto mr-auto" width="300" title="Row-Major VS Column-Major" alt="Row-Major VS Column-Major">
<br>

<p>In this blog post, I would like to discuss the difference between the row-major order and the column-major order, and their consequence for matrix multiplication performance.</p>
<h2 id="Row-Major-VS-Column-Major"><a href="#Row-Major-VS-Column-Major" class="headerlink" title="Row-Major VS Column-Major"></a>Row-Major VS Column-Major</h2><p>Given a matrix $A$ of shape $(M, N)$, if it is stored in row-major order, its leading dimension is $N$, and if it is stored in column-major order, its leading dimension is $M$.</p>
<p>To read $A^{\top}$ from the same piece of the memory in which $A$ was stored in row-major order with a leading dimension of $N$, we could just treat the matrix in the memory as if it were stored in column-major order and the leading dimension is still $N$.</p>
<p>To read $A^{\top}$ from the same piece of the memory in which $A$ was stored in column-major order with a leading dimension of $M$, we could just treat the matrix in the memory as if it were stored in row-major order and the leading dimension is still $M$.</p>
<!-- This makes transpose "free", since the program would just have to change the indexing for reading the values from the matrix without actually having to transpose the matrix in the memory. -->

<p>For example, we have a matrix $A$,</p>
<p>$$<br>\begin{align}<br>A &amp;=<br>\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\<br>4 &amp; 5 &amp; 6 \\<br>\end{bmatrix} \\<br>\end{align}<br>$$</p>
<p>If $A$ is stored in row-major order, the matrix values in the linear memory is $[1, 2, 3, 4, 5, 6]$.<br>If $A$ is stored in column-major order, the matrix values in the linear memory is $[1, 4, 2, 5, 3, 6]$.</p>
<p>The transpose of $A$, $A^{\top}$, is</p>
<p>$$<br>\begin{align}<br>A^{\top} &amp;=<br>\begin{bmatrix}<br>1 &amp; 4 \\<br>2 &amp; 5 \\<br>3 &amp; 6 \\<br>\end{bmatrix} \\<br>\end{align}<br>$$</p>
<p>If $A^{\top}$ is stored in row-major order, the matrix values in the linear memory is $[1, 4, 2, 5, 3, 6]$.<br>If $A^{\top}$ is stored in column-major order, the matrix values in the linear memory is $[1, 2, 3, 4, 5, 6]$.</p>
<p>It’s easy to see that $A$ is stored in row-major order is exactly the same as $A^{\top}$ is stored in column-major order in the memory and $A$ is stored in column-major order is exactly the same as $A^{\top}$ is stored in row-major order in the memory.</p>
<p>For a matrix $A$ stored in row-major order, reading rows of $A$ and reading columns of $A^{\top}$ are fast and cache-friendly, whereas reading columns of $A$ and reading rows of $A^{\top}$ are slow and invalids caching.</p>
<p>For a matrix $A$ stored in column-major order, reading columns of $A$ and reading rows of $A^{\top}$ are fast and cache-friendly, whereas reading rows of $A$ and reading columns of $A^{\top}$ are slow and invalids caching.</p>
<h2 id="Matrix-Multiplication"><a href="#Matrix-Multiplication" class="headerlink" title="Matrix Multiplication"></a>Matrix Multiplication</h2><p>The way of storing matrices in memory affects the performance of matrix multiplication on lots of processors, such as CPU and GPU. Usually, depending on whether the matrices for multiplications needs to be mathematically transposed for matrix multiplication, there are four ways of computing matrix multiplication, $C=AB$, $C=A^{\top}B$, $C=AB^{\top}$, and $C=A^{\top}B^{\top}$. Each of them performs better than others depending on the storage orderings of matrices $A$ and $B$, even though the theoretical MACs of the operations remain the same.</p>
<h3 id="C-AB"><a href="#C-AB" class="headerlink" title="$C=AB$"></a>$C=AB$</h3><p>Suppose a matrix $A$ is of shape $(M, K)$ and a matrix $B$ is of shape $(K, N)$, to compute $C = AB$ where $C$ is an matrix of shape $(M, N)$, each element in $C$ is an accumulated sum of one row of size $K$ in the matrix $A$ and one column of size $K$ in the matrix $B$.</p>
<p>Depending on the storage ordering of the two matrices, there are four scenarios.</p>
<table>
<thead>
<tr>
<th align="center">Matrix $A$ Storage Order</th>
<th align="center">Matrix $B$ Storage Order</th>
<th align="center">Matrix $A$ Row Reading</th>
<th align="center">Matrix $B$ Column Reading</th>
</tr>
</thead>
<tbody><tr>
<td align="center">column-major</td>
<td align="center">column-major</td>
<td align="center">slow</td>
<td align="center">fast</td>
</tr>
<tr>
<td align="center">column-major</td>
<td align="center">row-major</td>
<td align="center">slow</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">column-major</td>
<td align="center"><strong>fast</strong></td>
<td align="center"><strong>fast</strong></td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">row-major</td>
<td align="center">fast</td>
<td align="center">slow</td>
</tr>
</tbody></table>
<!-- * $A$ column-major, $B$ column-major -> reading $A$ rows slow, reading $B$ columns fast
* $A$ column-major, $B$ row-major -> reading $A$ rows slow, reading $B$ columns slow
* $A$ row-major, $B$ column-major -> **reading $A$ rows fast, reading $B$ columns fast**
* $A$ row-major, $B$ row-major -> reading $A$ rows fast, reading $B$ columns slow -->

<p>When $A$ is stored in row-major order and $B$ is stored in column-major order, reading both rows from $A$ and columns from $B$ are fast for matrix multiplication because of the caching mechanism of modern processors, and faster readings results in better performance, given the same amount of math to compute.</p>
<p>Therefore, the matrix multiplication $C=AB$ is more suitable for the situation where $A$ is stored in row-major order and $B$ is stored in column-major order.</p>
<h3 id="C-A-top-B"><a href="#C-A-top-B" class="headerlink" title="$C=A^{\top}B$"></a>$C=A^{\top}B$</h3><p>Suppose a matrix $A$ is of shape $(K, M)$ and a matrix $B$ is of shape $(K, N)$, to compute $C = A^{\top}B$ where $C$ is an matrix of shape $(M, N)$, each element in $C$ is an accumulated sum of one column of size $K$ in the matrix $A$ and one column of size $K$ in the matrix $B$.</p>
<p>Depending on the storage ordering of the two matrices, there are four scenarios.</p>
<table>
<thead>
<tr>
<th align="center">Matrix $A$ Storage Order</th>
<th align="center">Matrix $B$ Storage Order</th>
<th align="center">Matrix $A$ Column Reading</th>
<th align="center">Matrix $B$ Column Reading</th>
</tr>
</thead>
<tbody><tr>
<td align="center">column-major</td>
<td align="center">column-major</td>
<td align="center"><strong>fast</strong></td>
<td align="center"><strong>fast</strong></td>
</tr>
<tr>
<td align="center">column-major</td>
<td align="center">row-major</td>
<td align="center">fast</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">column-major</td>
<td align="center">slow</td>
<td align="center">fast</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">row-major</td>
<td align="center">slow</td>
<td align="center">slow</td>
</tr>
</tbody></table>
<!-- * $A$ column-major, $B$ column-major -> **reading $A$ columns fast, reading $B$ columns fast**
* $A$ column-major, $B$ row-major -> reading $A$ columns fast, reading $B$ columns slow
* $A$ row-major, $B$ column-major -> reading $A$ columns slow, reading $B$ columns fast
* $A$ row-major, $B$ row-major -> reading $A$ columns slow, reading $B$ columns slow -->

<p>When $A$ is stored in column-major order and $B$ is stored in column-major order, reading both columns from $A$ and columns from $B$ are fast for matrix multiplication because of the caching mechanism of modern processors, and faster readings results in better performance, given the same amount of math to compute.</p>
<p>Therefore, the matrix multiplication $C=A^{\top}B$ is more suitable for the situation where $A$ is stored in column-major order and $B$ is stored in column-major order.</p>
<h3 id="C-AB-top"><a href="#C-AB-top" class="headerlink" title="$C=AB^{\top}$"></a>$C=AB^{\top}$</h3><p>Suppose a matrix $A$ is of shape $(M, K)$ and a matrix $B$ is of shape $(N, K)$, to compute $C = AB^{\top}$ where $C$ is an matrix of shape $(M, N)$, each element in $C$ is an accumulated sum of one row of size $K$ in the matrix $A$ and one row of size $K$ in the matrix $B$.</p>
<p>Depending on the storage ordering of the two matrices, there are four scenarios.</p>
<table>
<thead>
<tr>
<th align="center">Matrix $A$ Storage Order</th>
<th align="center">Matrix $B$ Storage Order</th>
<th align="center">Matrix $A$ Row Reading</th>
<th align="center">Matrix $B$ Row Reading</th>
</tr>
</thead>
<tbody><tr>
<td align="center">column-major</td>
<td align="center">column-major</td>
<td align="center">slow</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">column-major</td>
<td align="center">row-major</td>
<td align="center">slow</td>
<td align="center">fast</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">column-major</td>
<td align="center">fast</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">row-major</td>
<td align="center"><strong>fast</strong></td>
<td align="center"><strong>fast</strong></td>
</tr>
</tbody></table>
<!-- * $A$ column-major, $B$ column-major -> reading $A$ rows slow, reading $B$ rows slow
* $A$ column-major, $B$ row-major -> reading $A$ rows slow, reading $B$ rows fast
* $A$ row-major, $B$ column-major -> reading $A$ rows fast, reading $B$ rows slow
* $A$ row-major, $B$ row-major -> **reading $A$ rows fast, reading $B$ rows fast** -->

<p>When $A$ is stored in row-major order and $B$ is stored in row-major order, reading both rows from $A$ and rows from $B$ are fast for matrix multiplication because of the caching mechanism of modern processors, and faster readings results in better performance, given the same amount of math to compute.</p>
<p>Therefore, the matrix multiplication $C=AB^{\top}$ is more suitable for the situation where $A$ is stored in row-major order and $B$ is stored in row-major order.</p>
<h3 id="C-A-top-B-top"><a href="#C-A-top-B-top" class="headerlink" title="$C=A^{\top}B^{\top}$"></a>$C=A^{\top}B^{\top}$</h3><p>Suppose a matrix $A$ is of shape $(K, M)$ and a matrix $B$ is of shape $(N, K)$, to compute $C = A^{\top}B^{\top}$ where $C$ is an matrix of shape $(M, N)$, each element in $C$ is an accumulated sum of one column of size $K$ in the matrix $A$ and one row of size $K$ in the matrix $B$.</p>
<p>Depending on the storage ordering of the two matrices, there are four scenarios.</p>
<table>
<thead>
<tr>
<th align="center">Matrix $A$ Storage Order</th>
<th align="center">Matrix $B$ Storage Order</th>
<th align="center">Matrix $A$ Column Reading</th>
<th align="center">Matrix $B$ Row Reading</th>
</tr>
</thead>
<tbody><tr>
<td align="center">column-major</td>
<td align="center">column-major</td>
<td align="center">fast</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">column-major</td>
<td align="center">row-major</td>
<td align="center"><strong>fast</strong></td>
<td align="center"><strong>fast</strong></td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">column-major</td>
<td align="center">slow</td>
<td align="center">slow</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">row-major</td>
<td align="center">slow</td>
<td align="center">fast</td>
</tr>
</tbody></table>
<!--
* $A$ column-major, $B$ column-major -> reading $A$ columns fast, reading $B$ rows slow
* $A$ column-major, $B$ row-major -> **reading $A$ columns fast, reading $B$ rows fast**
* $A$ row-major, $B$ column-major -> reading $A$ columns slow, reading $B$ rows slow
* $A$ row-major, $B$ row-major -> reading $A$ columns slow, reading $B$ rows fast
 -->

<p>When $A$ is stored in column-major order and $B$ is stored in row-major order, reading both columns from $A$ and rows from $B$ are fast for matrix multiplication because of the caching mechanism of modern processors, and faster readings results in better performance, given the same amount of math to compute.</p>
<p>Therefore, the matrix multiplication $C=A^{\top}B^{\top}$ is more suitable for the situation where $A$ is stored in column-major order and $B$ is stored in row-major order.</p>
<h2 id="Matrix-Multiplication-Preference"><a href="#Matrix-Multiplication-Preference" class="headerlink" title="Matrix Multiplication Preference"></a>Matrix Multiplication Preference</h2><p>The matrix multiplication preference for different combinations of the storage orders of matrices beings multiplied can be summarized as follows.</p>
<table>
<thead>
<tr>
<th align="center">Matrix $A$ Storage Order</th>
<th align="center">Matrix $B$ Storage Order</th>
<th align="center">Matrix Multiplication Preference</th>
</tr>
</thead>
<tbody><tr>
<td align="center">column-major</td>
<td align="center">column-major</td>
<td align="center">$C = A^{\top}B$</td>
</tr>
<tr>
<td align="center">column-major</td>
<td align="center">row-major</td>
<td align="center">$C = A^{\top}B^{\top}$</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">column-major</td>
<td align="center">$C = AB$</td>
</tr>
<tr>
<td align="center">row-major</td>
<td align="center">row-major</td>
<td align="center">$C = AB^{\top}$</td>
</tr>
</tbody></table>
<p>Because usually the all matrices in one software framework would use the same storage order, this means only $C = A^{\top}B$ and $C = AB^{\top}$ are preferred for those scenarios.</p>
<p>Optimizations can reduce the performance gap between the optimal matrix multiplication option and the other options, sometimes even to almost zero, depending on the implementation and the processor.</p>
<p>In addition, sometimes it’s not a good idea to physically transpose a matrix in memory just in order to use the most performant matrix multiplication option among the four, because the overhead of transposing a matrix in memory might be much larger than the difference between the four matrix multiplication options especially when they are all well optimized.</p>
<h2 id="Matrix-Multiplication-Benchmarks"><a href="#Matrix-Multiplication-Benchmarks" class="headerlink" title="Matrix Multiplication Benchmarks"></a>Matrix Multiplication Benchmarks</h2><p>Additionally, we could verify our analysis using C++ single-threaded naive implementations for matrix multiplication.</p>
<figure class="highlight c++"><figcaption><span>naive_mm.cpp</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdint&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_performance</span><span class="params">(std::function&lt;T(<span class="type">void</span>)&gt; bound_function,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span> num_repeats = <span class="number">100</span>, <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_warmups; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::chrono::steady_clock::time_point time_start&#123;</span><br><span class="line">        std::chrono::steady_clock::<span class="built_in">now</span>()&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_repeats; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>();</span><br><span class="line">    &#125;</span><br><span class="line">    std::chrono::steady_clock::time_point time_end&#123;</span><br><span class="line">        std::chrono::steady_clock::<span class="built_in">now</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> time_elapsed&#123;std::chrono::<span class="built_in">duration_cast</span>&lt;std::chrono::milliseconds&gt;(</span><br><span class="line">                          time_end - time_start)</span><br><span class="line">                          .<span class="built_in">count</span>()&#125;;</span><br><span class="line">    <span class="type">float</span> latency&#123;time_elapsed / <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(num_repeats)&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A and B are column-major matrices.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mm_a_col_major_b_col_major</span><span class="params">(T <span class="type">const</span>* A, T <span class="type">const</span>* B, T* C, <span class="type">uint32_t</span> m,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">uint32_t</span> n, <span class="type">uint32_t</span> k, <span class="type">uint32_t</span> lda,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">uint32_t</span> ldb, <span class="type">uint32_t</span> ldc, <span class="type">bool</span> is_A_transpose,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">bool</span> is_B_transpose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">uint32_t</span> ni&#123;<span class="number">0</span>&#125;; ni &lt; n; ++ni)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">uint32_t</span> mi&#123;<span class="number">0</span>&#125;; mi &lt; m; ++mi)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Compute C[mi, ni]</span></span><br><span class="line">            T accum&#123;<span class="number">0</span>&#125;;</span><br><span class="line">            <span class="comment">// A * B</span></span><br><span class="line">            <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[mi, ki] * B[ki, ni]</span></span><br><span class="line">                    accum += A[ki * lda + mi] * B[ni * ldb + ki];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// A^T * B</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[ki, mi] * B[ki, ni]</span></span><br><span class="line">                    accum += A[mi * lda + ki] * B[ni * ldb + ki];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// A * B^T</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[mi, ki] * B[ni, ki]</span></span><br><span class="line">                    accum += A[ki * lda + mi] * B[ki * ldb + ni];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// A^T * B^T</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[ki, mi] * B[ni, ki]</span></span><br><span class="line">                    accum += A[mi * lda + ki] * B[ki * ldb + ni];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            C[ni * ldc + mi] = accum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print_latency</span><span class="params">(<span class="type">float</span> latency)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; <span class="string">&quot;Latency: &quot;</span> &lt;&lt; latency</span><br><span class="line">              &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> num_repeats&#123;<span class="number">10</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> num_warmups&#123;<span class="number">10</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> M&#123;<span class="number">256</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> K&#123;<span class="number">256</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> N&#123;<span class="number">256</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_a</span><span class="params">(M * K)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_b</span><span class="params">(K * N)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_c</span><span class="params">(M * N)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span>* A&#123;matrix_a.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span>* B&#123;matrix_b.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">float</span>* C&#123;matrix_c.<span class="built_in">data</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_col_major_ld&#123;M&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_row_major_ld&#123;K&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_transpose_col_major_ld&#123;matrix_a_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_transpose_row_major_ld&#123;matrix_a_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_col_major_ld&#123;K&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_row_major_ld&#123;N&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_transpose_col_major_ld&#123;matrix_b_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_transpose_row_major_ld&#123;matrix_b_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_col_major_ld&#123;M&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_row_major_ld&#123;N&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_transpose_col_major_ld&#123;matrix_c_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_transpose_row_major_ld&#123;matrix_c_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;<span class="type">void</span>(<span class="type">void</span>)&gt; <span class="type">const</span> mm_a_col_major_b_col_major_a_b&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(mm_a_col_major_b_col_major&lt;<span class="type">float</span>&gt;, A, B, C, M, N, K,</span><br><span class="line">                  matrix_a_col_major_ld, matrix_b_col_major_ld,</span><br><span class="line">                  matrix_c_col_major_ld, <span class="literal">false</span>, <span class="literal">false</span>)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;<span class="type">void</span>(<span class="type">void</span>)&gt; <span class="type">const</span> mm_a_col_major_b_col_major_a_transpose_b&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(mm_a_col_major_b_col_major&lt;<span class="type">float</span>&gt;, A, B, C, M, N, K,</span><br><span class="line">                  matrix_a_transpose_col_major_ld, matrix_b_col_major_ld,</span><br><span class="line">                  matrix_c_col_major_ld, <span class="literal">true</span>, <span class="literal">false</span>)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;<span class="type">void</span>(<span class="type">void</span>)&gt; <span class="type">const</span></span><br><span class="line">        mm_a_col_major_b_col_major_a_transpose_b_transpose&#123;std::<span class="built_in">bind</span>(</span><br><span class="line">            mm_a_col_major_b_col_major&lt;<span class="type">float</span>&gt;, A, B, C, M, N, K,</span><br><span class="line">            matrix_a_transpose_col_major_ld, matrix_b_transpose_col_major_ld,</span><br><span class="line">            matrix_c_col_major_ld, <span class="literal">true</span>, <span class="literal">true</span>)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;<span class="type">void</span>(<span class="type">void</span>)&gt; <span class="type">const</span> mm_a_col_major_b_col_major_a_b_transpose&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(mm_a_col_major_b_col_major&lt;<span class="type">float</span>&gt;, A, B, C, M, N, K,</span><br><span class="line">                  matrix_a_col_major_ld, matrix_b_transpose_col_major_ld,</span><br><span class="line">                  matrix_c_col_major_ld, <span class="literal">false</span>, <span class="literal">true</span>)&#125;;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_b = <span class="built_in">measure_performance</span>(</span><br><span class="line">        mm_a_col_major_b_col_major_a_b, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_b);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A^T * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_transpose_b = <span class="built_in">measure_performance</span>(</span><br><span class="line">        mm_a_col_major_b_col_major_a_transpose_b, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_transpose_b);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_b_transpose = <span class="built_in">measure_performance</span>(</span><br><span class="line">        mm_a_col_major_b_col_major_a_b_transpose, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_b_transpose);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A^T * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_transpose_b_transpose =</span><br><span class="line">        <span class="built_in">measure_performance</span>(mm_a_col_major_b_col_major_a_transpose_b_transpose,</span><br><span class="line">                            num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_transpose_b_transpose);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">assert</span>(latency_a_transpose_b ==</span><br><span class="line">           std::<span class="built_in">min</span>(&#123;latency_a_b, latency_a_transpose_b, latency_a_b_transpose,</span><br><span class="line">                     latency_a_transpose_b_transpose&#125;));</span><br><span class="line">    <span class="built_in">assert</span>(latency_a_b_transpose ==</span><br><span class="line">           std::<span class="built_in">max</span>(&#123;latency_a_b, latency_a_transpose_b, latency_a_b_transpose,</span><br><span class="line">                     latency_a_transpose_b_transpose&#125;));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>We could see that given matrix $A$ and matrix $B$ are stored in column-major order, as expected, the performance of $C = A^{\top}B$ is the best and the performance of $C = AB^{\top}$ is the worst.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ g++ naive_mm.cpp -o naive_mm</span><br><span class="line">$ ./naive_mm</span><br><span class="line">C = A * B</span><br><span class="line">Latency: 45.400 ms</span><br><span class="line">C = A^T * B</span><br><span class="line">Latency: 32.500 ms</span><br><span class="line">C = A * B^T</span><br><span class="line">Latency: 57.800 ms</span><br><span class="line">C = A^T * B^T</span><br><span class="line">Latency: 48.300 ms</span><br></pre></td></tr></table></figure>

<p>Using multi-threaded optimized matrix multiplication implementations, such as the GEMM functions from the <a target="_blank" rel="noopener" href="https://developer.nvidia.com/cublas">cuBLAS</a> library, can eliminate the difference between the four options.</p>
<figure class="highlight c++"><figcaption><span>cublas_mm.cu</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cstdint&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;tuple&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cublas_v2.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUBLAS_ERROR(val) checkCuBlas((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">checkCuBlas</span><span class="params">(T err, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">                 <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (err != CUBLAS_STATUS_SUCCESS)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;cuBlas Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA_ERROR(val) checkCuda((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">checkCuda</span><span class="params">(T err, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">               <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; func &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_LAST_CUDA_ERROR() checkCudaLast(__FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">checkCudaLast</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file, <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaError_t <span class="type">const</span> err&#123;<span class="built_in">cudaGetLastError</span>()&#125;;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_cublas_performance</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    std::function&lt;cublasStatus_t(<span class="type">void</span>)&gt; bound_cublas_function,</span></span></span><br><span class="line"><span class="params"><span class="function">    cudaStream_t stream, <span class="type">int</span> num_repeats = <span class="number">100</span>, <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> time;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;stop));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_warmups; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUBLAS_ERROR</span>(<span class="built_in">bound_cublas_function</span>());</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(start, stream));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_repeats; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">CHECK_CUBLAS_ERROR</span>(<span class="built_in">bound_cublas_function</span>());</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(stop, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(stop));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency&#123;time / num_repeats&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print_latency</span><span class="params">(<span class="type">float</span> latency)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">3</span>) &lt;&lt; <span class="string">&quot;Latency: &quot;</span> &lt;&lt; latency</span><br><span class="line">              &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> num_repeats&#123;<span class="number">100</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> num_warmups&#123;<span class="number">100</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> M&#123;<span class="number">256</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> K&#123;<span class="number">256</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">uint32_t</span> N&#123;<span class="number">256</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* A&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    <span class="type">float</span>* B&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line">    <span class="type">float</span>* C&#123;<span class="literal">nullptr</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;A, M * K * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;B, K * N * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;C, M * N * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_col_major_ld&#123;M&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_row_major_ld&#123;K&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_transpose_col_major_ld&#123;matrix_a_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_a_transpose_row_major_ld&#123;matrix_a_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_col_major_ld&#123;K&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_row_major_ld&#123;N&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_transpose_col_major_ld&#123;matrix_b_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_b_transpose_row_major_ld&#123;matrix_b_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_col_major_ld&#123;M&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_row_major_ld&#123;N&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_transpose_col_major_ld&#123;matrix_c_row_major_ld&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_c_transpose_row_major_ld&#123;matrix_c_col_major_ld&#125;;</span><br><span class="line"></span><br><span class="line">    cublasHandle_t cublas_handle;</span><br><span class="line">    cudaStream_t stream;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line">    <span class="built_in">CHECK_CUBLAS_ERROR</span>(<span class="built_in">cublasCreate</span>(&amp;cublas_handle));</span><br><span class="line">    <span class="built_in">CHECK_CUBLAS_ERROR</span>(<span class="built_in">cublasSetStream</span>(cublas_handle, stream));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> alpha&#123;<span class="number">1.0</span>&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> beta&#123;<span class="number">0.0</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// cublasSgemm assumes column-major matrices.</span></span><br><span class="line">    std::function&lt;cublasStatus_t(<span class="type">void</span>)&gt; <span class="type">const</span> mm_a_col_major_b_col_major_a_b&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(cublasSgemm, cublas_handle, CUBLAS_OP_N, CUBLAS_OP_N, M, N, K,</span><br><span class="line">                  &amp;alpha, A, matrix_a_col_major_ld, B, matrix_b_col_major_ld,</span><br><span class="line">                  &amp;beta, C, matrix_c_col_major_ld)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;cublasStatus_t(<span class="type">void</span>)&gt; <span class="type">const</span></span><br><span class="line">        mm_a_col_major_b_col_major_a_transpose_b&#123;</span><br><span class="line">            std::<span class="built_in">bind</span>(cublasSgemm, cublas_handle, CUBLAS_OP_T, CUBLAS_OP_N, M,</span><br><span class="line">                      N, K, &amp;alpha, A, matrix_a_transpose_col_major_ld, B,</span><br><span class="line">                      matrix_b_col_major_ld, &amp;beta, C, matrix_c_col_major_ld)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;cublasStatus_t(<span class="type">void</span>)&gt; <span class="type">const</span></span><br><span class="line">        mm_a_col_major_b_col_major_a_transpose_b_transpose&#123;std::<span class="built_in">bind</span>(</span><br><span class="line">            cublasSgemm, cublas_handle, CUBLAS_OP_T, CUBLAS_OP_T, M, N, K,</span><br><span class="line">            &amp;alpha, A, matrix_a_transpose_col_major_ld, B,</span><br><span class="line">            matrix_b_transpose_col_major_ld, &amp;beta, C, matrix_c_col_major_ld)&#125;;</span><br><span class="line"></span><br><span class="line">    std::function&lt;cublasStatus_t(<span class="type">void</span>)&gt; <span class="type">const</span></span><br><span class="line">        mm_a_col_major_b_col_major_a_b_transpose&#123;std::<span class="built_in">bind</span>(</span><br><span class="line">            cublasSgemm, cublas_handle, CUBLAS_OP_N, CUBLAS_OP_T, M, N, K,</span><br><span class="line">            &amp;alpha, A, matrix_a_col_major_ld, B,</span><br><span class="line">            matrix_b_transpose_col_major_ld, &amp;beta, C, matrix_c_col_major_ld)&#125;;</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_b = <span class="built_in">measure_cublas_performance</span>(</span><br><span class="line">        mm_a_col_major_b_col_major_a_b, stream, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_b);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A^T * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_transpose_b =</span><br><span class="line">        <span class="built_in">measure_cublas_performance</span>(mm_a_col_major_b_col_major_a_transpose_b,</span><br><span class="line">                                   stream, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_transpose_b);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_b_transpose =</span><br><span class="line">        <span class="built_in">measure_cublas_performance</span>(mm_a_col_major_b_col_major_a_b_transpose,</span><br><span class="line">                                   stream, num_repeats, num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_b_transpose);</span><br><span class="line"></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;C = A^T * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_a_transpose_b_transpose = <span class="built_in">measure_cublas_performance</span>(</span><br><span class="line">        mm_a_col_major_b_col_major_a_transpose_b_transpose, stream, num_repeats,</span><br><span class="line">        num_warmups);</span><br><span class="line">    <span class="built_in">print_latency</span>(latency_a_transpose_b_transpose);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(A));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(B));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(C));</span><br><span class="line">    <span class="built_in">CHECK_CUBLAS_ERROR</span>(<span class="built_in">cublasDestroy</span>(cublas_handle));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamDestroy</span>(stream));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>With the highly optimized implementations, there is almost no difference between the four options.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc cublas_mm.cu -o cublas_mm -lcublas</span><br><span class="line">$ ./cublas_mm</span><br><span class="line">C = A * B</span><br><span class="line">Latency: 0.008 ms</span><br><span class="line">C = A^T * B</span><br><span class="line">Latency: 0.010 ms</span><br><span class="line">C = A * B^T</span><br><span class="line">Latency: 0.009 ms</span><br><span class="line">C = A^T * B^T</span><br><span class="line">Latency: 0.008 ms</span><br></pre></td></tr></table></figure>

<!--

## DDDDDDDDDDDDDDDDD


A: K x M
B: K x N
C: M x N

A^T x B


A x B -> 4 storage combinations

$A$ column-major, $B$ column-major -> **reading $A$ columns fast, reading $B$ columns fast**
$A$ column-major, $B$ row-major -> reading $A$ columns fast, reading $B$ columns slow
$A$ row-major, $B$ column-major -> reading $A$ columns slow, reading $B$ columns fast
$A$ row-major, $B$ row-major -> reading $A$ columns slow, reading $B$ columns slow






A: M x K
B: N x K

A x B^T


A x B -> 4 storage combinations

$A$ column-major, $B$ column-major -> reading $A$ rows slow, reading $B$ rows slow
$A$ column-major, $B$ row-major -> reading $A$ rows slow, reading $B$ rows fast
$A$ row-major, $B$ column-major -> reading $A$ rows fast, reading $B$ rows slow
$A$ row-major, $B$ row-major -> reading $A$ rows fast, reading $B$ rows fast






Because in a program, usually all the different matrices will be stored on the memory using exactly the same order, either row-major or column major. The performance of matrix multiplication functions

A x B
A^T x B
A x B^T
A^T x B^T

will be different.

Suppose all the matrices are stored using row-major order, A x B^T is the matrix multiplication function that will give the best performance.

Similarly, suppose all the matrices are stored using col-major order, A^T x B is the matrix multiplication function that will give the best performance.


Optimizations can reduce the performance gap between different matrix multiplication functions, but usually the gap would still be non-zero. (Show an cublas example)



## DDDDDDDDDDDDDDDDDD














A: M x K
B: N x K

A x B^T


A: K x M
B: N x K

A^T x B^T






















## BBBBBBB

But it's not absolutely "free" on GPU. Suppose we have an INT8 matrix of shape M x N, M = 32, N = 12.

If it were saved as column-major we have a warp of 32 threads reading column by column, because of the coalesced memory access, reading the full matrix would take N transactions. If we want to read its transpose by treating it as row-major,


## AAAAAAAAAAA

tell the function to read the memory as column-major and the leading dimension is N.






Conversely, if the user is given a pointer to a piece of memory containing a matrix data,
 -->

<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Row-_and_column-major_order">Row- and Column-Major Order - Wikipedia</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Row-Major VS Column-Major</p><p><a href="https://leimao.github.io/blog/Row-Major-VS-Column-Major/">https://leimao.github.io/blog/Row-Major-VS-Column-Major/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>05-12-2023</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>05-12-2023</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/CPP/">CPP,</a> </div><div><a class="link-muted" rel="tag" href="/tags/CUDA/">CUDA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/Computer-Architecture/">Computer Architecture,</a> </div><div><a class="link-muted" rel="tag" href="/tags/Memory/">Memory </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/leimao" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/life/Ardenwood-Historic-Farm/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Ardenwood Historic Farm 参观</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/essay/%E6%BC%AB%E9%95%BF%E7%9A%84%E5%AD%A3%E8%8A%82/"><span class="level-item">漫长的季节</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://leimao.github.io/blog/Row-Major-VS-Column-Major/';
            this.page.identifier = '/blog/Row-Major-VS-Column-Major/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'leimao-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/leimao" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/leimao" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchaleimao"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukeleimao@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Row-Major-VS-Column-Major"><span class="level-left"><span class="level-item">2</span><span class="level-item">Row-Major VS Column-Major</span></span></a></li><li><a class="level is-mobile" href="#Matrix-Multiplication"><span class="level-left"><span class="level-item">3</span><span class="level-item">Matrix Multiplication</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#C-AB"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">$C=AB$</span></span></a></li><li><a class="level is-mobile" href="#C-A-top-B"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">$C=A^{\top}B$</span></span></a></li><li><a class="level is-mobile" href="#C-AB-top"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">$C=AB^{\top}$</span></span></a></li><li><a class="level is-mobile" href="#C-A-top-B-top"><span class="level-left"><span class="level-item">3.4</span><span class="level-item">$C=A^{\top}B^{\top}$</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Matrix-Multiplication-Preference"><span class="level-left"><span class="level-item">4</span><span class="level-item">Matrix Multiplication Preference</span></span></a></li><li><a class="level is-mobile" href="#Matrix-Multiplication-Benchmarks"><span class="level-left"><span class="level-item">5</span><span class="level-item">Matrix Multiplication Benchmarks</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">6</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=leimaogithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>