<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>CUDA Constant Memory - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="CUDA Constant Memory Usages and Caveats"><meta property="og:type" content="blog"><meta property="og:title" content="CUDA Constant Memory"><meta property="og:url" content="https://levanliu.github.io/blog/CUDA-Constant-Memory/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="CUDA Constant Memory Usages and Caveats"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta property="article:published_time" content="2023-12-01T08:00:00.000Z"><meta property="article:modified_time" content="2023-12-01T08:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="NVIDIA"><meta property="article:tag" content="GPU"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta property="twitter:creator" content="@matchalevanliu"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukelevanliu"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://levanliu.github.io/blog/CUDA-Constant-Memory/"},"headline":"CUDA Constant Memory","image":["https://levanliu.github.io/images/favicon/android-chrome-512x512.png"],"datePublished":"2023-12-01T08:00:00.000Z","dateModified":"2023-12-01T08:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://levanliu.github.io/images/favicon/android-chrome-512x512.png"}},"description":"CUDA Constant Memory Usages and Caveats"}</script><link rel="canonical" href="https://levanliu.github.io/blog/CUDA-Constant-Memory/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/levanliu"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/levanliu/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:levanliugithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">CUDA Constant Memory</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-12-01T08:00:00.000Z" title="2023-12-01T08:00:00.000Z">12-01-2023</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-12-01T08:00:00.000Z" title="2023-12-01T08:00:00.000Z">12-01-2023</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 14 minutes read (About 2033 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>CUDA constant memory is a special memory space on the device. It’s cached and read-only.</p>
<p>There are some caveats when using constant memory. In this post, we will discuss the usages and caveats of constant memory.</p>
<h2 id="Constant-Memory"><a href="#Constant-Memory" class="headerlink" title="Constant Memory"></a>Constant Memory</h2><p>There is a total of 64 KB constant memory on a device. The constant memory space is cached. As a result, a read from constant memory costs one memory read from device memory only on a cache miss; otherwise, it just costs one read from the constant cache. Accesses to different addresses by threads within a warp are serialized, thus the cost scales linearly with the number of unique addresses read by all threads within a warp. As such, the constant cache is best when threads in the same warp accesses only a few distinct locations. If all threads of a warp access the same location, then constant memory can be as fast as a register access.</p>
<h2 id="Constant-Memory-Usage-and-Performance"><a href="#Constant-Memory-Usage-and-Performance" class="headerlink" title="Constant Memory Usage and Performance"></a>Constant Memory Usage and Performance</h2><p>In the following example, we perform additions for an array. One of the constant input arrays is stored on global memory, and the other constant input arrays is stored on global memory or constant memory. We compare the performance of accessing constant memory and global memory under different access patterns.</p>
<figure class="highlight cpp"><figcaption><span>add_constant.cu</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">check</span><span class="params">(cudaError_t err, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; func &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_LAST_CUDA_ERROR() checkLast(__FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">checkLast</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file, <span class="type">const</span> <span class="type">int</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaError_t <span class="type">const</span> err&#123;<span class="built_in">cudaGetLastError</span>()&#125;;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_performance</span><span class="params">(std::function&lt;T(cudaStream_t)&gt; bound_function,</span></span></span><br><span class="line"><span class="params"><span class="function">                          cudaStream_t stream, <span class="type">unsigned</span> <span class="type">int</span> num_repeats = <span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">unsigned</span> <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> time;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;stop));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_warmups; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(start, stream));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_repeats; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(stop, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(stop));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency&#123;time / num_repeats&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Use all the constant memory.</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> N&#123;<span class="number">64U</span> * <span class="number">1024U</span> / <span class="built_in">sizeof</span>(<span class="type">int</span>)&#125;;</span><br><span class="line">__constant__ <span class="type">int</span> const_values[N];</span><br><span class="line"></span><br><span class="line"><span class="comment">// Magic number for generating the pseudo-random access pattern.</span></span><br><span class="line"><span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> magic_number&#123;<span class="number">1357U</span>&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">enum struct</span> <span class="title class_">AccessPattern</span></span><br><span class="line">&#123;</span><br><span class="line">    OneAccessPerBlock,</span><br><span class="line">    OneAccessPerWarp,</span><br><span class="line">    OneAccessPerThread,</span><br><span class="line">    PseudoRandom</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">add_constant_cpu</span><span class="params">(<span class="type">int</span>* sums, <span class="type">int</span> <span class="type">const</span>* inputs, <span class="type">int</span> <span class="type">const</span>* values,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">unsigned</span> <span class="type">int</span> num_sums, <span class="type">unsigned</span> <span class="type">int</span> num_values,</span></span></span><br><span class="line"><span class="params"><span class="function">                      <span class="type">unsigned</span> <span class="type">int</span> block_size, AccessPattern access_pattern)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0U</span>&#125;; i &lt; num_sums; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> block_id&#123;i / block_size&#125;;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> thread_id&#123;i % block_size&#125;;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> warp_id&#123;thread_id / <span class="number">32U</span>&#125;;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">int</span> index&#123;<span class="number">0U</span>&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (access_pattern)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">case</span> AccessPattern::OneAccessPerBlock:</span><br><span class="line">                index = block_id % num_values;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> AccessPattern::OneAccessPerWarp:</span><br><span class="line">                index = warp_id % num_values;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> AccessPattern::OneAccessPerThread:</span><br><span class="line">                index = thread_id % num_values;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            <span class="keyword">case</span> AccessPattern::PseudoRandom:</span><br><span class="line">                index = (thread_id * magic_number) % num_values;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        sums[i] = inputs[i] + values[index];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_constant_global_memory</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">int</span>* sums, <span class="type">int</span> <span class="type">const</span>* inputs, <span class="type">int</span> <span class="type">const</span>* values, <span class="type">unsigned</span> <span class="type">int</span> num_sums,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">unsigned</span> <span class="type">int</span> num_values,</span></span></span><br><span class="line"><span class="params"><span class="function">    AccessPattern access_pattern = AccessPattern::OneAccessPerBlock)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> i&#123;blockIdx.x * blockDim.x + threadIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> block_id&#123;blockIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> thread_id&#123;threadIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> warp_id&#123;threadIdx.x / warpSize&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> index&#123;<span class="number">0U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (access_pattern)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerBlock:</span><br><span class="line">            index = block_id % num_values;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerWarp:</span><br><span class="line">            index = warp_id % num_values;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerThread:</span><br><span class="line">            index = thread_id % num_values;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::PseudoRandom:</span><br><span class="line">            index = (thread_id * magic_number) % num_values;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i &lt; num_sums)</span><br><span class="line">    &#123;</span><br><span class="line">        sums[i] = inputs[i] + values[index];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add_constant_global_memory</span><span class="params">(<span class="type">int</span>* sums, <span class="type">int</span> <span class="type">const</span>* inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       <span class="type">int</span> <span class="type">const</span>* values, <span class="type">unsigned</span> <span class="type">int</span> num_sums,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       <span class="type">unsigned</span> <span class="type">int</span> num_values,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       <span class="type">unsigned</span> <span class="type">int</span> block_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       AccessPattern access_pattern,</span></span></span><br><span class="line"><span class="params"><span class="function">                                       cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    add_constant_global_memory&lt;&lt;&lt;(num_sums + block_size - <span class="number">1</span>) / block_size,</span><br><span class="line">                                 block_size, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">        sums, inputs, values, num_sums, num_values, access_pattern);</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">add_constant_constant_memory</span><span class="params">(<span class="type">int</span>* sums, <span class="type">int</span> <span class="type">const</span>* inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                             <span class="type">unsigned</span> <span class="type">int</span> num_sums,</span></span></span><br><span class="line"><span class="params"><span class="function">                                             AccessPattern access_pattern)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> i&#123;blockIdx.x * blockDim.x + threadIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> block_id&#123;blockIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> thread_id&#123;threadIdx.x&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> <span class="type">const</span> warp_id&#123;threadIdx.x / warpSize&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> index&#123;<span class="number">0U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">switch</span> (access_pattern)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerBlock:</span><br><span class="line">            index = block_id % N;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerWarp:</span><br><span class="line">            index = warp_id % N;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::OneAccessPerThread:</span><br><span class="line">            index = thread_id % N;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">        <span class="keyword">case</span> AccessPattern::PseudoRandom:</span><br><span class="line">            index = (thread_id * magic_number) % N;</span><br><span class="line">            <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i &lt; num_sums)</span><br><span class="line">    &#123;</span><br><span class="line">        sums[i] = inputs[i] + const_values[index];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_add_constant_constant_memory</span><span class="params">(<span class="type">int</span>* sums, <span class="type">int</span> <span class="type">const</span>* inputs,</span></span></span><br><span class="line"><span class="params"><span class="function">                                         <span class="type">unsigned</span> <span class="type">int</span> num_sums,</span></span></span><br><span class="line"><span class="params"><span class="function">                                         <span class="type">unsigned</span> <span class="type">int</span> block_size,</span></span></span><br><span class="line"><span class="params"><span class="function">                                         AccessPattern access_pattern,</span></span></span><br><span class="line"><span class="params"><span class="function">                                         cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    add_constant_constant_memory&lt;&lt;&lt;(num_sums + block_size - <span class="number">1</span>) / block_size,</span><br><span class="line">                                   block_size, <span class="number">0</span>, stream&gt;&gt;&gt;(</span><br><span class="line">        sums, inputs, num_sums, access_pattern);</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">parse_args</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv, AccessPattern&amp; access_pattern,</span></span></span><br><span class="line"><span class="params"><span class="function">                <span class="type">unsigned</span> <span class="type">int</span>&amp; block_size, <span class="type">unsigned</span> <span class="type">int</span>&amp; num_sums)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (argc &lt; <span class="number">4</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Usage: &quot;</span> &lt;&lt; argv[<span class="number">0</span>]</span><br><span class="line">                  &lt;&lt; <span class="string">&quot; &lt;access pattern&gt; &lt;block size&gt; &lt;number of sums&gt;&quot;</span></span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    std::string <span class="type">const</span> access_pattern_str&#123;argv[<span class="number">1</span>]&#125;;</span><br><span class="line">    <span class="keyword">if</span> (access_pattern_str == <span class="string">&quot;one_access_per_block&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        access_pattern = AccessPattern::OneAccessPerBlock;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (access_pattern_str == <span class="string">&quot;one_access_per_warp&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        access_pattern = AccessPattern::OneAccessPerWarp;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (access_pattern_str == <span class="string">&quot;one_access_per_thread&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        access_pattern = AccessPattern::OneAccessPerThread;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> (access_pattern_str == <span class="string">&quot;pseudo_random&quot;</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        access_pattern = AccessPattern::PseudoRandom;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;Invalid access pattern: &quot;</span> &lt;&lt; access_pattern_str</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    block_size = std::<span class="built_in">stoi</span>(argv[<span class="number">2</span>]);</span><br><span class="line">    num_sums = std::<span class="built_in">stoi</span>(argv[<span class="number">3</span>]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">int</span> argc, <span class="type">char</span>** argv)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> num_warmups&#123;<span class="number">100U</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">unsigned</span> <span class="type">int</span> num_repeats&#123;<span class="number">100U</span>&#125;;</span><br><span class="line"></span><br><span class="line">    AccessPattern access_pattern&#123;AccessPattern::OneAccessPerBlock&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> block_size&#123;<span class="number">1024U</span>&#125;;</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> num_sums&#123;<span class="number">12800000U</span>&#125;;</span><br><span class="line">    <span class="comment">// Modify access pattern, block size and number of sums from command line.</span></span><br><span class="line">    <span class="built_in">parse_args</span>(argc, argv, access_pattern, block_size, num_sums);</span><br><span class="line"></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span> h_values[N];</span><br><span class="line">    <span class="comment">// Initialize values on host memory.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0U</span>&#125;; i &lt; N; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        h_values[i] = i;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// Initialize values on global memory.</span></span><br><span class="line">    <span class="type">int</span>* d_values;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMallocAsync</span>(&amp;d_values, N * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(d_values, h_values, N * <span class="built_in">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">                                     cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="comment">// Initialize values on constant memory.</span></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyToSymbolAsync</span>(const_values, h_values,</span><br><span class="line">                                             N * <span class="built_in">sizeof</span>(<span class="type">int</span>), <span class="number">0</span>,</span><br><span class="line">                                             cudaMemcpyHostToDevice, stream));</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">inputs</span><span class="params">(num_sums, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="type">int</span>* h_inputs&#123;inputs.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int</span>* d_inputs_for_constant;</span><br><span class="line">    <span class="type">int</span>* d_inputs_for_global;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMallocAsync</span>(&amp;d_inputs_for_constant,</span><br><span class="line">                                     num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(</span><br><span class="line">        <span class="built_in">cudaMallocAsync</span>(&amp;d_inputs_for_global, num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(d_inputs_for_constant, h_inputs,</span><br><span class="line">                                     num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">                                     cudaMemcpyHostToDevice, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(d_inputs_for_global, h_inputs,</span><br><span class="line">                                     num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">                                     cudaMemcpyHostToDevice, stream));</span><br><span class="line"></span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">reference_sums</span><span class="params">(num_sums, <span class="number">0</span>)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">sums_from_constant</span><span class="params">(num_sums, <span class="number">1</span>)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int</span>&gt; <span class="title">sums_from_global</span><span class="params">(num_sums, <span class="number">2</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* h_reference_sums&#123;reference_sums.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int</span>* h_sums_from_constant&#123;sums_from_constant.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int</span>* h_sums_from_global&#123;sums_from_global.<span class="built_in">data</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">int</span>* d_sums_from_constant;</span><br><span class="line">    <span class="type">int</span>* d_sums_from_global;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(</span><br><span class="line">        <span class="built_in">cudaMallocAsync</span>(&amp;d_sums_from_constant, num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(</span><br><span class="line">        <span class="built_in">cudaMallocAsync</span>(&amp;d_sums_from_global, num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>), stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Synchronize.</span></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Compute reference sums on CPU.</span></span><br><span class="line">    <span class="built_in">add_constant_cpu</span>(h_reference_sums, h_inputs, h_values, num_sums, N,</span><br><span class="line">                     block_size, access_pattern);</span><br><span class="line">    <span class="comment">// Compute reference sums on GPU using global memory.</span></span><br><span class="line">    <span class="built_in">launch_add_constant_global_memory</span>(d_sums_from_global, d_inputs_for_global,</span><br><span class="line">                                      d_values, num_sums, N, block_size,</span><br><span class="line">                                      access_pattern, stream);</span><br><span class="line">    <span class="comment">// Compute reference sums on GPU using constant memory.</span></span><br><span class="line">    <span class="built_in">launch_add_constant_constant_memory</span>(d_sums_from_constant,</span><br><span class="line">                                        d_inputs_for_constant, num_sums,</span><br><span class="line">                                        block_size, access_pattern, stream);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy results from device to host.</span></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(h_sums_from_constant, d_sums_from_constant,</span><br><span class="line">                                     num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">                                     cudaMemcpyDeviceToHost, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpyAsync</span>(h_sums_from_global, d_sums_from_global,</span><br><span class="line">                                     num_sums * <span class="built_in">sizeof</span>(<span class="type">int</span>),</span><br><span class="line">                                     cudaMemcpyDeviceToHost, stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Synchronize.</span></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Verify results.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i&#123;<span class="number">0U</span>&#125;; i &lt; num_sums; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (h_reference_sums[i] != h_sums_from_constant[i])</span><br><span class="line">        &#123;</span><br><span class="line">            std::cerr &lt;&lt; <span class="string">&quot;Error at index &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; for constant memory.&quot;</span></span><br><span class="line">                      &lt;&lt; std::endl;</span><br><span class="line">            std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">if</span> (h_reference_sums[i] != h_sums_from_global[i])</span><br><span class="line">        &#123;</span><br><span class="line">            std::cerr &lt;&lt; <span class="string">&quot;Error at index &quot;</span> &lt;&lt; i &lt;&lt; <span class="string">&quot; for global memory.&quot;</span></span><br><span class="line">                      &lt;&lt; std::endl;</span><br><span class="line">            std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Measure performance.</span></span><br><span class="line">    std::function&lt;<span class="type">void</span>(cudaStream_t)&gt; bound_function_constant_memory&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(launch_add_constant_constant_memory, d_sums_from_constant,</span><br><span class="line">                  d_inputs_for_constant, num_sums, block_size, access_pattern,</span><br><span class="line">                  std::placeholders::_1)&#125;;</span><br><span class="line">    std::function&lt;<span class="type">void</span>(cudaStream_t)&gt; bound_function_global_memory&#123;</span><br><span class="line">        std::<span class="built_in">bind</span>(launch_add_constant_global_memory, d_sums_from_global,</span><br><span class="line">                  d_inputs_for_global, d_values, num_sums, N, block_size,</span><br><span class="line">                  access_pattern, std::placeholders::_1)&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_constant_memory&#123;<span class="built_in">measure_performance</span>(</span><br><span class="line">        bound_function_constant_memory, stream, num_repeats, num_warmups)&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency_global_memory&#123;<span class="built_in">measure_performance</span>(</span><br><span class="line">        bound_function_global_memory, stream, num_repeats, num_warmups)&#125;;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Latency for Add using constant memory: &quot;</span></span><br><span class="line">              &lt;&lt; latency_constant_memory &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Latency for Add using global memory: &quot;</span></span><br><span class="line">              &lt;&lt; latency_global_memory &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamDestroy</span>(stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_values));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_inputs_for_constant));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_inputs_for_global));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_sums_from_constant));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_sums_from_global));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>The program was compiled and executed on an NVIDIA RTX 3090 GPU.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc add_constant.cu -o add_constant</span><br></pre></td></tr></table></figure>

<p>If we have 12800000 adds to perform using 1024 threads per block.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./add_constant one_access_per_block 1024 12800000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.151798 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.171404 ms</span><br><span class="line">$ ./add_constant one_access_per_warp 1024 12800000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.164012 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.189501 ms</span><br><span class="line">$ ./add_constant one_access_per_thread 1024 12800000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.281967 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.164649 ms</span><br><span class="line">$ ./add_constant pseudo_random 1024 12800000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 1.2925 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.159621 ms</span><br></pre></td></tr></table></figure>

<p>If we have 128000 adds to perform using 1024 threads per block.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ./add_constant one_access_per_block 1024 128000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.00289792 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.00323584 ms</span><br><span class="line">$ ./add_constant one_access_per_warp 1024 128000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.00315392 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.00359392 ms</span><br><span class="line">$ ./add_constant one_access_per_thread 1024 128000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.00596992 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.00383264 ms</span><br><span class="line">$ ./add_constant pseudo_random 1024 128000</span><br><span class="line">Latency <span class="keyword">for</span> Add using constant memory: 0.0215347 ms</span><br><span class="line">Latency <span class="keyword">for</span> Add using global memory: 0.00482304 ms</span><br></pre></td></tr></table></figure>

<p>In both cases, we could see that accessing constant memory is ~10% faster than accessing global memory if the it’s one access per block or one access per warp. If it’s one access per thread, then accessing constant memory is ~70% slower than accessing global memory. If it’s pseudo random access, then accessing constant memory is ~800% slower than accessing global memory.</p>
<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>To use constant memory, it’s important to roughly know the access pattern. If the access pattern is one access per block or one access per warp, which is typically used in broadcast, then constant memory is a good choice. If the access pattern is one access per thread or even pseudo random, then constant memory is a very bad choice.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#device-memory-spaces">Device Memory Spaces</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#constant-memory">Constant Memory</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#constant">Constant Specifier</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>CUDA Constant Memory</p><p><a href="https://levanliu.github.io/blog/CUDA-Constant-Memory/">https://levanliu.github.io/blog/CUDA-Constant-Memory/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>12-01-2023</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>12-01-2023</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/CUDA/">CUDA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/NVIDIA/">NVIDIA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/GPU/">GPU </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/levanliu" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/life/Winchester-Mystery-House/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Winchester Mystery House 参观</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/essay/%E7%BA%A2%E8%89%B2%E8%AD%A6%E6%88%922%E7%BD%91%E9%A1%B5%E7%89%88/"><span class="level-item">红色警戒 2 网页版</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://levanliu.github.io/blog/CUDA-Constant-Memory/';
            this.page.identifier = '/blog/CUDA-Constant-Memory/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'levanliu-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/levanliu" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/levanliu" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/levanliu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchalevanliu"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukelevanliu@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Constant-Memory"><span class="level-left"><span class="level-item">2</span><span class="level-item">Constant Memory</span></span></a></li><li><a class="level is-mobile" href="#Constant-Memory-Usage-and-Performance"><span class="level-left"><span class="level-item">3</span><span class="level-item">Constant Memory Usage and Performance</span></span></a></li><li><a class="level is-mobile" href="#Conclusions"><span class="level-left"><span class="level-item">4</span><span class="level-item">Conclusions</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">5</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=levanliugithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/levanliu"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>