<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>CUDA Tensor Layouts for Convolution - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="Motivations for Different Tensor Layouts"><meta property="og:type" content="blog"><meta property="og:title" content="CUDA Tensor Layouts for Convolution"><meta property="og:url" content="https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="Motivations for Different Tensor Layouts"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-06-04-CUDA-Convolution-Tensor-Layouts/convo-tensor.svg"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-06-04-CUDA-Convolution-Tensor-Layouts/gemm-dim-equiv.svg"><meta property="article:published_time" content="2023-06-04T07:00:00.000Z"><meta property="article:modified_time" content="2023-06-04T07:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="Accelerated Computing"><meta property="article:tag" content="CUDA"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://levanliu.github.io/images/blog/2023-06-04-CUDA-Convolution-Tensor-Layouts/convo-tensor.svg"><meta property="twitter:creator" content="@matchaleimao"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukeleimao"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/"},"headline":"CUDA Tensor Layouts for Convolution","image":[],"datePublished":"2023-06-04T07:00:00.000Z","dateModified":"2023-06-04T07:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://levanliu.github.io/images/favicon/android-chrome-512x512.png"}},"description":"Motivations for Different Tensor Layouts"}</script><link rel="canonical" href="https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/leimao/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:leimaogithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">CUDA Tensor Layouts for Convolution</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-06-04T07:00:00.000Z" title="2023-06-04T07:00:00.000Z">06-04-2023</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-06-04T07:00:00.000Z" title="2023-06-04T07:00:00.000Z">06-04-2023</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 13 minutes read (About 1958 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>There are commonly two layouts for the activation tensors involved in the convolution operations in neural networks, <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-880/developer-guide/index.html#nchw-layout-x32">NCHW</a>, <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-880/developer-guide/index.html#nhwc-layout-x32">NHWC</a>, and <a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-880/developer-guide/index.html#nc32hw32-layout-x32">NC/xHWx</a>.</p>
<p>In general, the performance of convolution using NHWC is much faster than using NCHW. The NC/xHWx layout is an variant of NHWC that is prepared for NVIDIA Tensor Core operations.</p>
<p>In this blog post, I would like to discuss how to perform convolution on GPU and why NHWC and NC/xHWx activation tensor layouts are much more favored than the NCHW activation tensor layout for convolutional neural network inference. </p>
<h2 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h2><p>The description of convolution in neural networks can be found in the documentation of many deep learning frameworks, such as <a target="_blank" rel="noopener" href="https://pytorch.org/docs/1.13/generated/torch.nn.Conv2d.html?highlight=conv2d#torch.nn.Conv2d">PyTorch</a>.</p>
<h3 id="Convolution-Dimensions"><a href="#Convolution-Dimensions" class="headerlink" title="Convolution Dimensions"></a>Convolution Dimensions</h3><p>The 2D convolution operation in neural networks consists of an input activation tensor, a filter tensor, an optional bias tensor, and an output activation tensor. We will ignore the bias tensor in this article since it is usually simple to deal with.</p>
<p>The input and output activation tensors and the filter tensor are all 4D tensors that consist of four dimensions. We use N to describe the batch dimension for the input and output tensors. We use C, H, W to describe the number of channels, the spatial height, and the spatial width of the input activation tensor. In order to distinguish the output activation tensor from the input activation tensor, we use K, P, Q to describe the number of channels, the spatial height, and the spatial width of the output activation tensor instead. The filter height and width are described using R and S, respectively.</p>
<img src="/images/blog/2023-06-04-CUDA-Convolution-Tensor-Layouts/convo-tensor.svg" class="box px-0 py-0 ml-auto mr-auto" width="480" title="Convolution Dimension Naming Conventions" alt="Convolution Dimension Naming Conventions">
<br>


<h3 id="Implicit-GEMM-for-Convolution"><a href="#Implicit-GEMM-for-Convolution" class="headerlink" title="Implicit GEMM for Convolution"></a>Implicit GEMM for Convolution</h3><p>In my previous article <a href="/blog/Fast-Fourier-Transform-Convolution/">“Fast Fourier Transform for Convolution”</a>, I described how to perform convolution using the asymptotically faster fast Fourier transform. But this technique is still not the most common way of performing convolution nowadays on GPU and it is out of the scope of this article.</p>
<p>In my previous article <a href="blog/Convolution-Transposed-Convolution-As-Matrix-Multiplication/">“Convolution and Transposed Convolution as Matrix Multiplication”</a>, I described how to perform convolution using matrix multiplication in which the activation tensors are dense but the filter tensor is sparse. </p>
<p>On GPUs, convolution is usually performed using a method called implicit GEMM. GEMM stands for general matrix multiplication. The difference between the implicit GEMM method that I am about to describe and the method I described in the article <a href="blog/Convolution-Transposed-Convolution-As-Matrix-Multiplication/">“Convolution and Transposed Convolution as Matrix Multiplication”</a> is that all the matrices used in the implicit GEMM method are dense matrices.</p>
<p>The implicit GEMM method for convolution can be described using the following figure. We will focus on the forward propagation (a) only as the gradient updates (b and c) usually do not happen in the neural network inference.</p>
<img src="/images/blog/2023-06-04-CUDA-Convolution-Tensor-Layouts/gemm-dim-equiv.svg" class="box px-0 py-0 ml-auto mr-auto" width="600" title="The Virtual Matrices Used in the Implicit GEMM Methods for Convolution" alt="The Virtual Matrices Used in the Implicit GEMM Methods for Convolution">
<br>

<p>Theoretically, if we transpose, expand and reshape the input activation from a 4D tensor of shape $(N, C, H, W)$ to a 2D tensor of shape $(NPQ, CRS)$, transpose and reshape the weight tensor from 4D $(K, C, S, R)$ to 2D $(CRS, K)$, multiply the two tensors, the 2D output tensor is a tensor of shape $(NPQ, K)$ and can be further transposed to the 4D output activation tensor of shape $(N, K, P, Q)$. For example, suppose $N = 1$, $C = K = 1$, $H = W = 3$, $R = S = 2$, $P = Q = 2$ (the convolution stride is 1 and the padding is “valid”). Because there is only one input channel, the only spatial feature in the input activation tensor is a matrix and its values can be assumed to be</p>
<p>$$<br>\begin{bmatrix}<br>1 &amp; 2 &amp; 3 \\<br>4 &amp; 5 &amp; 6 \\<br>7 &amp; 8 &amp; 9 \\<br>\end{bmatrix}<br>$$</p>
<p>The reconstructed input activation matrix will be of shape $(4, 4)$ and its values are</p>
<p>$$<br>\begin{bmatrix}<br>1 &amp; 2 &amp; 4 &amp; 5 \\<br>2 &amp; 3 &amp; 5 &amp; 6 \\<br>4 &amp; 5 &amp; 7 &amp; 8 \\<br>5 &amp; 6 &amp; 8 &amp; 9 \\<br>\end{bmatrix}<br>$$</p>
<p>The problem of this theoretical formulation is that a new matrices always need to be constructed during inference because the output activation tensor, which will usually be used as the input activation tensor for the next convolution layer, is not of the reconstructed format. Even though the theoretical ration between the number of values in the reconstructed input activation matrix and the number of values in the original input activation tensor is $\frac{PQRS}{HW}$ which sometimes can be 1, constructing such a new matrix is not a no-op and will introduce overhead in computing, not to mention consuming additional memory when this ratio is high. Therefore, in practice, this reconstructed input activation matrix is never constructed in the implicit GEMM method for convolution. The values are read from the input activation tensor of its original layout instead.</p>
<h2 id="NVIDIA-Tensor-Core"><a href="#NVIDIA-Tensor-Core" class="headerlink" title="NVIDIA Tensor Core"></a>NVIDIA Tensor Core</h2><p>NVIDIA Tensor Core performs small matrix multiplications to accelerate GEMM with extremely high throughput. For example, NVIDIA Tensor Core could perform 16×16×16 GEMM, 16x16 and 16x16 matrix multiplication (and accumulation) for half precision floating point data on a warp basis. Fundamentally, the mathematical motivation of Tensor Core GEMM acceleration has been described in my previous article <a href="/blog/CUDA-Matrix-Multiplication/#Matrix-Multiplication-Decomposition">CUDA Matrix Multiplication</a>, although not explicitly at that time.</p>
<p>$$<br>\mathbf{A} =<br>\begin{bmatrix}<br>\mathbf{A}_{1,1}^{d \times d} &amp; \mathbf{A}_{1,2}^{d \times d} &amp; \cdots &amp; \mathbf{A}_{1,n/d}^{d \times d} \\<br>\mathbf{A}_{2,1}^{d \times d} &amp; \mathbf{A}_{2,2}^{d \times d} &amp; \cdots &amp; \mathbf{A}_{2,n/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\mathbf{A}_{m/d,1}^{d \times d} &amp; \mathbf{A}_{m/d,2}^{d \times d} &amp; \cdots &amp; \mathbf{A}_{m/d,n/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>\mathbf{B} =<br>\begin{bmatrix}<br>\mathbf{B}_{1,1}^{d \times d} &amp; \mathbf{B}_{1,2}^{d \times d} &amp; \cdots &amp; \mathbf{B}_{1,p/d}^{d \times d} \\<br>\mathbf{B}_{2,1}^{d \times d} &amp; \mathbf{B}_{2,2}^{d \times d} &amp; \cdots &amp; \mathbf{B}_{2,p/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\mathbf{B}_{n/d,1}^{d \times d} &amp; \mathbf{B}_{n/d,2}^{d \times d} &amp; \cdots &amp; \mathbf{B}_{n/d,p/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>\mathbf{C} =<br>\begin{bmatrix}<br>\mathbf{C}_{1,1}^{d \times d} &amp; \mathbf{C}_{1,2}^{d \times d} &amp; \cdots &amp; \mathbf{C}_{1,p/d}^{d \times d} \\<br>\mathbf{C}_{2,1}^{d \times d} &amp; \mathbf{C}_{2,2}^{d \times d} &amp; \cdots &amp; \mathbf{C}_{2,p/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>\mathbf{C}_{m/d,1}^{d \times d} &amp; \mathbf{C}_{m/d,2}^{d \times d} &amp; \cdots &amp; \mathbf{C}_{m/d,p/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>\mathbf{C}_{i,j}^{d \times d} = \sum_{k=1}^{n/d} \mathbf{A}_{i,k}^{d \times d} \mathbf{B}_{k,j}^{d \times d}<br>$$</p>
<p>Basically, by decomposing the large matrix multiplication into smaller matrix multiplications and accumulation and caching the small matrices, we could make GEMM extremely math bound. Specifically, the small matrices $\mathbf{A}_{i,k}^{d \times d}$ and $\mathbf{B}_{k,j}^{d \times d}$ are cached in the registers of a warp, and each warp computes a $\mathbf{C}_{i,j}^{d \times d}$ using Tensor Core by iterating the small matrice multiplication and accumulation $\frac{n}{d}$ times. </p>
<h2 id="Tensor-Layouts"><a href="#Tensor-Layouts" class="headerlink" title="Tensor Layouts"></a>Tensor Layouts</h2><p>Now that we have some basic idea of how convolution is performed on GPU via implicit GEMM. Let’s check the impact of different activation layouts on the performance of convolution.</p>
<h3 id="NCHW"><a href="#NCHW" class="headerlink" title="NCHW"></a>NCHW</h3><p>In the NCHW layout, C is not the fastest dimension. This means, even without assuming the implementation, getting the entire channels from the input activations for implicit GEMM ($CRS$) needs to stride lots of times, which significantly reduced the valid memory throughput on GPU. For example, suppose the input activation tensor has $N = 1$, $C = 256$, and $H = W = 128$, to get an entire channel from the spatial indices $(12, 35)$ for 1x1 convolution, the slicing operation we will perform for the first sample is $X[1, :, 12, 35]$. Under the hood, getting an entire channel of size $C = 256$ needs to stride $C = 256$ times of size $HW = 16384$ and this invalids the coalesced reading of the data from the DRAM on GPU.</p>
<p>Therefore, the NCHW layout is not favored for the implicit GEMM for convolution.</p>
<h3 id="NHWC"><a href="#NHWC" class="headerlink" title="NHWC"></a>NHWC</h3><p>In the NHWC layout, C becomes the fastest dimension. Unlike NCHW slicing for the C dimension, the NHWC slicing for the C dimension can be fully coalesced from the DRAM.</p>
<p>Therefore, the NHWC layout is favored over the NCHW layout for the implicit GEMM for convolution.</p>
<h3 id="NC-xHWx"><a href="#NC-xHWx" class="headerlink" title="NC/xHWx"></a>NC/xHWx</h3><p>To take the advantage of NVIDIA Tensor Core, the “virtual” reconstructed input activation matrix needs to be divided in a way that is compatible with the Tensor Core GEMM. This requires the “virtual” reconstructed input activation matrix to be padded (with zeros) so that $CSR$ could be divided by the small matrix dimension requirements from Tensor Core operations. The NHWC layout provides no such guarantee therefore applying the NHWC tensor for Tensor Core GEMM requires padding during the runtime and therefore is a little bit cumbersome to use with Tensor Core.</p>
<p>The NC/xHWx layout is always padded to x elements for the fastest (C) dimension, where x is usually the Tensor Core GEMM dimension requirement. Therefore, it is immediately ready to be used with Tensor Core.</p>
<p>One might ask, is there a NHWC variant layout whose C dimension is not divided like the NC/xHWx layout but padded to x elements according to the Tensor Core GEMM dimension requirement. The answer is yes and using that layout can be very performant for the implicit GEMM method for convolution as well. My educative guess for the reason why NC/xHWx is slightly more often seen than the padded NHWC layout is that the indexing and slicing for the NC/xHWx layout might be more natural in the implementation than that for the padded NHWC layout. For example, using the padded NHWC layout, the indexing and slicing of the input activation tensor would be like this.</p>
<!-- 
$$
\begin{align}
&X[1, 0:4, 0:4, 0:16] \\\\
&X[1, 0:4, 0:4, 16:32] \\\\
&X[1, 0:4, 0:4, 32:48] \\\\
&\vdots
\end{align}
$$ -->


<p>$$<br>\begin{align}<br>&amp;X[1, 0:4, 0:4, 0:16] \\<br>&amp;X[1, 0:4, 0:4, 16:32] \\<br>&amp;X[1, 0:4, 0:4, 32:48] \\<br>\end{align}<br>$$</p>
<p>Using the NC/16HW16 layout instead, the indexing and slicing of the input activation tensor to get the equivalent matrices would be like this.</p>
<!-- 
$$
\begin{align}
&X[1, 0, 0:4, 0:4, :] \\\\
&X[1, 1, 0:4, 0:4, :] \\\\
&X[1, 2, 0:4, 0:4, :] \\\\
&\vdots
\end{align}
$$
 -->



<p>$$<br>\begin{align}<br>&amp;X[1, 0, 0:4, 0:4, :] \\<br>&amp;X[1, 1, 0:4, 0:4, :] \\<br>&amp;X[1, 2, 0:4, 0:4, :] \\<br>\end{align}<br>$$</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/performance/dl-performance-convolutional/index.html">NVIDIA Convolutional Layers User’s Guide</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-853/api/c_api/namespacenvinfer1.html#ac3e115b1a2b1e578e8221ef99d27cd45">TensorRT Tensor Formats</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/deeplearning/cudnn/archives/cudnn-880/developer-guide/index.html#data-layout-formats">cuDNN Tensor Layout Formats</a></li>
<li><a href="/blog/NVIDIA-Tensor-Core-Programming/">NVIDIA Tensor Core Programming</a></li>
<li><a href="/blog/Convolution-Transposed-Convolution-As-Matrix-Multiplication/">Convolution and Transposed Convolution as Matrix Multiplication</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/using-tensor-cores-in-cuda-fortran/">Using Tensor Cores in CUDA Fortran</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/11.7.0/cuda-c-programming-guide/index.html#device-memory-accesses">CUDA Device Memory Access</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/11.7.0/cuda-c-best-practices-guide/index.html#coalesced-access-to-global-memory">Coalesced Access to Global Memory</a></li>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">Programming Tensor Cores in CUDA 9</a></li>
<li><a target="_blank" rel="noopener" href="https://github.com/NVIDIA-developer-blog/code-samples/blob/9b28573ad056560d3a862fea6ddbe4643809e59c/posts/tensor-cores/simpleTensorCoreGEMM.cu">Simple Tensor Core GEMM</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>CUDA Tensor Layouts for Convolution</p><p><a href="https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/">https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>06-04-2023</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>06-04-2023</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/Accelerated-Computing/">Accelerated Computing,</a> </div><div><a class="link-muted" rel="tag" href="/tags/CUDA/">CUDA </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/leimao" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/essay/%E5%B0%8F%E5%AD%A6%E8%AE%A1%E7%AE%97%E6%9C%BA%E8%AF%BE/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">我小学时的计算机课</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/essay/PIS%E7%82%92%E8%82%A1%E5%A4%B1%E8%B4%A5%E5%9B%9E%E5%BD%92%E7%9B%B4%E6%92%AD/"><span class="level-item">PIS 炒股失败回归直播</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://levanliu.github.io/blog/CUDA-Convolution-Tensor-Layouts/';
            this.page.identifier = '/blog/CUDA-Convolution-Tensor-Layouts/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'leimao-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/leimao" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/leimao" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchaleimao"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukeleimao@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Convolution"><span class="level-left"><span class="level-item">2</span><span class="level-item">Convolution</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Convolution-Dimensions"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Convolution Dimensions</span></span></a></li><li><a class="level is-mobile" href="#Implicit-GEMM-for-Convolution"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Implicit GEMM for Convolution</span></span></a></li></ul></li><li><a class="level is-mobile" href="#NVIDIA-Tensor-Core"><span class="level-left"><span class="level-item">3</span><span class="level-item">NVIDIA Tensor Core</span></span></a></li><li><a class="level is-mobile" href="#Tensor-Layouts"><span class="level-left"><span class="level-item">4</span><span class="level-item">Tensor Layouts</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#NCHW"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">NCHW</span></span></a></li><li><a class="level is-mobile" href="#NHWC"><span class="level-left"><span class="level-item">4.2</span><span class="level-item">NHWC</span></span></a></li><li><a class="level is-mobile" href="#NC-xHWx"><span class="level-left"><span class="level-item">4.3</span><span class="level-item">NC/xHWx</span></span></a></li></ul></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">5</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=leimaogithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>