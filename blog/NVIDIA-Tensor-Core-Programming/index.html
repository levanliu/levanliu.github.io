<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>NVIDIA Tensor Core Programming - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="Fast Matrix Multiplication and Accumulation on GPU"><meta property="og:type" content="blog"><meta property="og:title" content="NVIDIA Tensor Core Programming"><meta property="og:url" content="https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="Fast Matrix Multiplication and Accumulation on GPU"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/turing-tensor-core-math.png"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/Turing-Tensor-Core-New-Diag-White-Background.jpg"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/ga100-full-gpu-128-sms.png"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/a100-sm.png"><meta property="article:published_time" content="2023-05-18T07:00:00.000Z"><meta property="article:modified_time" content="2023-12-27T08:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="Accelerated Computing"><meta property="article:tag" content="CUDA"><meta property="article:tag" content="NVIDIA"><meta property="article:tag" content="C++"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/turing-tensor-core-math.png"><meta property="twitter:creator" content="@matchaleimao"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukeleimao"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/"},"headline":"NVIDIA Tensor Core Programming","image":["https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/turing-tensor-core-math.png","https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/Turing-Tensor-Core-New-Diag-White-Background.jpg","https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/ga100-full-gpu-128-sms.png","https://levanliu.github.io/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/a100-sm.png"],"datePublished":"2023-05-18T07:00:00.000Z","dateModified":"2023-12-27T08:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://levanliu.github.io/images/favicon/android-chrome-512x512.png"}},"description":"Fast Matrix Multiplication and Accumulation on GPU"}</script><link rel="canonical" href="https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/leimao/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:leimaogithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">NVIDIA Tensor Core Programming</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-05-18T07:00:00.000Z" title="2023-05-18T07:00:00.000Z">05-18-2023</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-12-27T08:00:00.000Z" title="2023-12-27T08:00:00.000Z">12-27-2023</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 28 minutes read (About 4239 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>NVIDIA Tensor Cores are dedicated accelerators for general matrix multiplication (GEMM) operations on NVIDIA GPUs since the Volta architecture. Because the artificial intelligence computations are usually dominated by GEMM operations, NVIDIA Tensor Core is critical for accelerating the artificial intelligence applications.</p>
<h2 id="NVIDIA-Tensor-Core"><a href="#NVIDIA-Tensor-Core" class="headerlink" title="NVIDIA Tensor Core"></a>NVIDIA Tensor Core</h2><p>NVIDIA Tensor Cores are specialized in performing the GEMM operations in mixed precision, i.e., the GEMM input matrices are in lower precision whereas the GEMM output matrix are in high precision. The mixed precision training and inference are the key techniques for accelerating the training and inference of neural networks.</p>
<img src="/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/turing-tensor-core-math.png" class="box px-0 py-0 ml-auto mr-auto" width="960" title="NVIDIA Tensor Core GEMM Math" alt="NVIDIA Tensor Core GEMM Math">
<br>

<p>Because NVIDIA Tensor Cores are specifically designed for GEMM, the GEMM throughput using NVIDIA Tensor Core is incredibly much higher than what can be achieved using NVIDIA CUDA Cores which are more suitable for more general parallel programming.</p>
<img src="/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/Turing-Tensor-Core-New-Diag-White-Background.jpg" class="box px-0 py-0 ml-auto mr-auto" width="960" title="NVIDIA GEMM Throughput Turing Tensor Core VS Pascal CUDA Core" alt="NVIDIA GEMM Throughput Turing Tensor Core VS Pascal CUDA Core">
<br>

<p>For the NVIDIA Ampere architecture, each SM has 4 Tensor Cores. In particular, <a target="_blank" rel="noopener" href="https://www.nvidia.com/en-us/data-center/a100/">NVIDIA A100 GPU</a> has 108 streaming multiprocessors (SMs) which accounts for 432 Tensor Cores in total.</p>
<img src="/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/ga100-full-gpu-128-sms.png" class="box px-0 py-0 ml-auto mr-auto" width="960" title="NVIDIA GA100 Full GPU with 128 SMs" alt="NVIDIA GA100 Full GPU with 128 SMs">
<br>

<img src="/images/blog/2023-05-18-NVIDIA-Tensor-Core-Programming/a100-sm.png" class="box px-0 py-0 ml-auto mr-auto" width="512" title="Each NVIDIA Ampere SM Has 4 Tensor Cores" alt="Each NVIDIA Ampere SM Has 4 Tensor Cores">
<br>

<p>NVIDIA Tensor Cores are fully programmable. The Tensor Core programming API at the warp level has been declared in the <code>mma.h</code> header under the <code>nvcuda::wmma</code> namespace.</p>
<h2 id="NVIDIA-Tensor-Core-Programming"><a href="#NVIDIA-Tensor-Core-Programming" class="headerlink" title="NVIDIA Tensor Core Programming"></a>NVIDIA Tensor Core Programming</h2><h3 id="Matrix-Multiplication-Decomposition"><a href="#Matrix-Multiplication-Decomposition" class="headerlink" title="Matrix Multiplication Decomposition"></a>Matrix Multiplication Decomposition</h3><!--
CUDA allows the user to program Tensor Core GEMM operations $D = AB + C$, and its transposed variants $D = A^{\top}B + C$, $D = AB^{\top} + C$, $D = A^{\top}B^{\top} + C$, at the warp level. -->

<p>NVIDIA CUDA allows the user to program Tensor Core GEMM operations $D = AB + C$ at the warp level. While each Tensor Core could only perform matrix multiplication of some <a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/12.0.1/cuda-c-programming-guide/#element-types-and-matrix-sizes">specific small sizes for different data types</a>, as discussed in my previous article <a href="/blog/CUDA-Matrix-Multiplication/">“CUDA Matrix Multiplication”</a>, large GEMM can be divided into multiple small GEMMs and accumulation.</p>
<p>Given a GEMM operation $D = AB + C$, where $D \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{m \times k}$, $B \in \mathbb{R}^{k \times n}$, $C \in \mathbb{R}^{m \times n}$, the matrices could be divided into smaller matrices.</p>
<p>$$<br>A =<br>\begin{bmatrix}<br>A_{1,1}^{d \times d} &amp; A_{1,2}^{d \times d} &amp; \cdots &amp; A_{1,k/d}^{d \times d} \\<br>A_{2,1}^{d \times d} &amp; A_{2,2}^{d \times d} &amp; \cdots &amp; A_{2,k/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>A_{m/d,1}^{d \times d} &amp; A_{m/d,2}^{d \times d} &amp; \cdots &amp; A_{m/d,k/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>B =<br>\begin{bmatrix}<br>B_{1,1}^{d \times d} &amp; B_{1,2}^{d \times d} &amp; \cdots &amp; B_{1,n/d}^{d \times d} \\<br>B_{2,1}^{d \times d} &amp; B_{2,2}^{d \times d} &amp; \cdots &amp; B_{2,n/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>B_{k/d,1}^{d \times d} &amp; B_{k/d,2}^{d \times d} &amp; \cdots &amp; B_{k/d,n/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>C =<br>\begin{bmatrix}<br>C_{1,1}^{d \times d} &amp; C_{1,2}^{d \times d} &amp; \cdots &amp; C_{1,n/d}^{d \times d} \\<br>C_{2,1}^{d \times d} &amp; C_{2,2}^{d \times d} &amp; \cdots &amp; C_{2,n/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>C_{m/d,1}^{d \times d} &amp; C_{m/d,2}^{d \times d} &amp; \cdots &amp; C_{m/d,n/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>$$<br>D =<br>\begin{bmatrix}<br>D_{1,1}^{d \times d} &amp; D_{1,2}^{d \times d} &amp; \cdots &amp; D_{1,n/d}^{d \times d} \\<br>D_{2,1}^{d \times d} &amp; D_{2,2}^{d \times d} &amp; \cdots &amp; D_{2,n/d}^{d \times d} \\<br>\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\<br>D_{m/d,1}^{d \times d} &amp; D_{m/d,2}^{d \times d} &amp; \cdots &amp; D_{m/d,n/d}^{d \times d} \\<br>\end{bmatrix}<br>$$</p>
<p>Each small matrix in $D$ is computed as multiple small GEMMs and accumulation.</p>
<p>$$<br>D_{i_m,i_n}^{d \times d} = \sum_{i_k=1}^{k/d} A_{i_m,i_k}^{d \times d} B_{i_k,i_n}^{d \times d}<br>$$</p>
<p>In my previous article <a href="/blog/CUDA-Matrix-Multiplication/">“CUDA Matrix Multiplication”</a>, I used CUDA Core and CUDA shared memory to perform the above mathematics and each thread block computes one $D_{i_m,i_n}^{d \times d}$. This time instead, I will use Tensor Core to compute exactly the same mathematics where each warp computes one $D_{i_m,i_n}^{d \times d}$. More specifically, each warp computes a $16 \times 16 \times 16$ GEMM resulting in a $16 \times 16$ tile in the $D$ matrix, i.e., $d = 16$.</p>
<h3 id="Matrix-Multiplication-Implementation-Using-NVIDIA-Tensor-Core"><a href="#Matrix-Multiplication-Implementation-Using-NVIDIA-Tensor-Core" class="headerlink" title="Matrix Multiplication Implementation Using NVIDIA Tensor Core"></a>Matrix Multiplication Implementation Using NVIDIA Tensor Core</h3><p>In this implementation, we will use Tensor Core to perform GEMM operations using HMMA (half matrix multiplication and accumulation) and IMMA (integer matrix multiplication and accumulation) instructions. In addition, four different types of GEMM which involves transposed matrix multiplications have been implemented and verified.</p>
<ul>
<li>$D = AB + C$, where $D \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{m \times k}$, $B \in \mathbb{R}^{k \times n}$, $C \in \mathbb{R}^{m \times n}$</li>
<li>$D = A^{\top}B + C$, where $D \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $B \in \mathbb{R}^{k \times n}$, $C \in \mathbb{R}^{m \times n}$</li>
<li>$D = AB^{\top} + C$, where $D \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{m \times k}$, $B \in \mathbb{R}^{n \times k}$, $C \in \mathbb{R}^{m \times n}$</li>
<li>$D = A^{\top}B^{\top} + C$, where $D \in \mathbb{R}^{m \times n}$, $A \in \mathbb{R}^{k \times m}$, $B \in \mathbb{R}^{n \times k}$, $C \in \mathbb{R}^{m \times n}$</li>
</ul>
<p>In this implementation, we will mainly focus on the matrix multiplication part in the GEMM operation by treating the $C = 0$.</p>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cassert&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;chrono&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iomanip&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;random&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;utility&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;mma.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_CUDA_ERROR(val) check((val), #val, __FILE__, __LINE__)</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">check</span><span class="params">(T err, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> func, <span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file,</span></span></span><br><span class="line"><span class="params"><span class="function">           <span class="type">int</span> <span class="type">const</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; <span class="string">&quot; &quot;</span> &lt;&lt; func &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> CHECK_LAST_CUDA_ERROR() checkLast(__FILE__, __LINE__)</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">checkLast</span><span class="params">(<span class="type">const</span> <span class="type">char</span>* <span class="type">const</span> file, <span class="type">int</span> <span class="type">const</span> line)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaError_t <span class="type">const</span> err&#123;<span class="built_in">cudaGetLastError</span>()&#125;;</span><br><span class="line">    <span class="keyword">if</span> (err != cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        std::cerr &lt;&lt; <span class="string">&quot;CUDA Runtime Error at: &quot;</span> &lt;&lt; file &lt;&lt; <span class="string">&quot;:&quot;</span> &lt;&lt; line</span><br><span class="line">                  &lt;&lt; std::endl;</span><br><span class="line">        std::cerr &lt;&lt; <span class="built_in">cudaGetErrorString</span>(err) &lt;&lt; std::endl;</span><br><span class="line">        std::<span class="built_in">exit</span>(EXIT_FAILURE);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">class</span> <span class="title class_">T</span>&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">measure_performance</span><span class="params">(std::function&lt;T(cudaStream_t)&gt; bound_function,</span></span></span><br><span class="line"><span class="params"><span class="function">                          cudaStream_t stream, <span class="type">int</span> num_repeats = <span class="number">100</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">                          <span class="type">int</span> num_warmups = <span class="number">100</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    cudaEvent_t start, stop;</span><br><span class="line">    <span class="type">float</span> time;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventCreate</span>(&amp;stop));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_warmups; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(start, stream));</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> i&#123;<span class="number">0</span>&#125;; i &lt; num_repeats; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">bound_function</span>(stream);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventRecord</span>(stop, stream));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventSynchronize</span>(stop));</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventElapsedTime</span>(&amp;time, start, stop));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(start));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaEventDestroy</span>(stop));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> latency&#123;time / num_repeats&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> latency;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// All the data in the matrices are stored in a column-major order,</span></span><br><span class="line"><span class="comment">// which is the consistent with most of the cuBLAS GEMM APIs.</span></span><br><span class="line"><span class="comment">// For matrix A of shape M x N, the leading dimension is M.</span></span><br><span class="line"><span class="comment">// For matrix A that is transposed and is of shape N x M,</span></span><br><span class="line"><span class="comment">// the leading dimension is N.</span></span><br><span class="line"><span class="comment">// Matrix A: M x K, or K x N (if transposed).</span></span><br><span class="line"><span class="comment">// Matrix B: K x M, or M x K (if transposed).</span></span><br><span class="line"><span class="comment">// Matrix C: M x N.</span></span><br><span class="line"><span class="comment">// WMMA_FRAG_LAYOUT_A: nvcuda::wmma::row_major if A is</span></span><br><span class="line"><span class="comment">// transposed, otherwise nvcuda::wmma::col_major.</span></span><br><span class="line"><span class="comment">// WMMA_FRAG_LAYOUT_B: nvcuda::wmma::row_major if B is</span></span><br><span class="line"><span class="comment">// transposed, otherwise nvcuda::wmma::col_major.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2, <span class="type">int</span> WMMA_M, <span class="type">int</span> WMMA_N, <span class="type">int</span> WMMA_K,</span><br><span class="line">          <span class="keyword">typename</span> WMMA_FRAG_LAYOUT_A, <span class="keyword">typename</span> WMMA_FRAG_LAYOUT_B&gt;</span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">wmma_gemm_a_col_major_b_col_major</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    T1 <span class="type">const</span>* A, T1 <span class="type">const</span>* B, T2* C, <span class="type">uint32_t</span> m, <span class="type">uint32_t</span> n, <span class="type">uint32_t</span> k,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">uint32_t</span> lda, <span class="type">uint32_t</span> ldb, <span class="type">uint32_t</span> ldc, <span class="type">bool</span> is_A_transpose,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="type">bool</span> is_B_transpose, <span class="type">float</span> alpha, <span class="type">float</span> beta)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Tile using a 2D grid.</span></span><br><span class="line">    <span class="comment">// Determine the warp 2D index.</span></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> warpM&#123;(blockIdx.x * blockDim.x + threadIdx.x) / warpSize&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> warpN&#123;blockIdx.y * blockDim.y + threadIdx.y&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Declare the fragments.</span></span><br><span class="line">    nvcuda::wmma::fragment&lt;nvcuda::wmma::matrix_a, WMMA_M, WMMA_N, WMMA_K, T1,</span><br><span class="line">                           WMMA_FRAG_LAYOUT_A&gt;</span><br><span class="line">        a_frag&#123;&#125;;</span><br><span class="line">    nvcuda::wmma::fragment&lt;nvcuda::wmma::matrix_b, WMMA_M, WMMA_N, WMMA_K, T1,</span><br><span class="line">                           WMMA_FRAG_LAYOUT_B&gt;</span><br><span class="line">        b_frag&#123;&#125;;</span><br><span class="line">    nvcuda::wmma::fragment&lt;nvcuda::wmma::accumulator, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                           T2&gt;</span><br><span class="line">        acc_frag&#123;&#125;;</span><br><span class="line">    nvcuda::wmma::fragment&lt;nvcuda::wmma::accumulator, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                           T2&gt;</span><br><span class="line">        c_frag&#123;&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Make sure the accumulator starts from 0.</span></span><br><span class="line">    nvcuda::wmma::<span class="built_in">fill_fragment</span>(acc_frag, <span class="built_in">static_cast</span>&lt;T2&gt;(<span class="number">0</span>));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Loop over K.</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ki += WMMA_K)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// Determine the first element of the mma matrices on the linear memory.</span></span><br><span class="line">        <span class="comment">// Matrix A mma matrix</span></span><br><span class="line">        <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_a_row_idx&#123;is_A_transpose ? ki</span><br><span class="line">                                                           : warpM * WMMA_M&#125;;</span><br><span class="line">        <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_a_col_idx&#123;is_A_transpose ? warpM * WMMA_M</span><br><span class="line">                                                           : ki&#125;;</span><br><span class="line">        <span class="comment">// Matrix B mma matrix</span></span><br><span class="line">        <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_b_row_idx&#123;is_B_transpose ? warpN * WMMA_N</span><br><span class="line">                                                           : ki&#125;;</span><br><span class="line">        <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_b_col_idx&#123;is_B_transpose ? ki</span><br><span class="line">                                                           : warpN * WMMA_N&#125;;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// Bounds checking</span></span><br><span class="line">        <span class="keyword">if</span> (matrix_mma_a_row_idx &lt; (is_A_transpose ? k : m) &amp;&amp;</span><br><span class="line">            matrix_mma_a_col_idx &lt; (is_A_transpose ? m : k) &amp;&amp;</span><br><span class="line">            matrix_mma_b_row_idx &lt; (is_B_transpose ? n : k) &amp;&amp;</span><br><span class="line">            matrix_mma_b_col_idx &lt; (is_B_transpose ? k : n))</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Determine the memory address of the first element of the mma</span></span><br><span class="line">            <span class="comment">// matrices. Notice that all the matrices are assumed to be</span></span><br><span class="line">            <span class="comment">// column-major. Therefore, the indexing is different from the</span></span><br><span class="line">            <span class="comment">// row-major indexing that we commonly see.</span></span><br><span class="line">            T1 <span class="type">const</span>* matrix_mma_a_mptr&#123;A + matrix_mma_a_row_idx +</span><br><span class="line">                                        matrix_mma_a_col_idx * lda&#125;;</span><br><span class="line">            T1 <span class="type">const</span>* matrix_mma_b_mptr&#123;B + matrix_mma_b_row_idx +</span><br><span class="line">                                        matrix_mma_b_col_idx * ldb&#125;;</span><br><span class="line">            <span class="comment">// Load the mma matrix inputs.</span></span><br><span class="line">            nvcuda::wmma::<span class="built_in">load_matrix_sync</span>(a_frag, matrix_mma_a_mptr, lda);</span><br><span class="line">            nvcuda::wmma::<span class="built_in">load_matrix_sync</span>(b_frag, matrix_mma_b_mptr, ldb);</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Perform the matrix multiplication</span></span><br><span class="line">            nvcuda::wmma::<span class="built_in">mma_sync</span>(acc_frag, a_frag, b_frag, acc_frag);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Load in the current value of c, scale it by beta, and add this our result</span></span><br><span class="line">    <span class="comment">// scaled by alpha.</span></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_c_row_idx&#123;warpM * WMMA_M&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_mma_c_col_idx&#123;warpN * WMMA_N&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (matrix_mma_c_row_idx &lt; m &amp;&amp; matrix_mma_c_col_idx &lt; n)</span><br><span class="line">    &#123;</span><br><span class="line">        T2* matrix_mma_c_mptr&#123;C + matrix_mma_c_row_idx +</span><br><span class="line">                              matrix_mma_c_col_idx * ldc&#125;;</span><br><span class="line">        nvcuda::wmma::<span class="built_in">load_matrix_sync</span>(c_frag, matrix_mma_c_mptr, ldc,</span><br><span class="line">                                       nvcuda::wmma::mem_col_major);</span><br><span class="line">        <span class="comment">// Let the compiler figure out how to do the elementwise operation.</span></span><br><span class="line">        <span class="comment">// Such elementwise operation can be scaling, accumulation,</span></span><br><span class="line">        <span class="comment">// quantization, etc.</span></span><br><span class="line">        <span class="comment">// https://docs.nvidia.com/cuda/archive/12.0.1/cuda-c-programming-guide/#id40</span></span><br><span class="line">        <span class="comment">// Be careful when dealing with the integer types.</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="type">uint32_t</span> i = <span class="number">0</span>; i &lt; c_frag.num_elements; i++)</span><br><span class="line">        &#123;</span><br><span class="line">            c_frag.x[i] = alpha * acc_frag.x[i] + beta * c_frag.x[i];</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// Store the output</span></span><br><span class="line">        nvcuda::wmma::<span class="built_in">store_matrix_sync</span>(matrix_mma_c_mptr, c_frag, ldc,</span><br><span class="line">                                        nvcuda::wmma::mem_col_major);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_wmma_mm</span><span class="params">(T1 <span class="type">const</span>* A, T1 <span class="type">const</span>* B, T2* C, <span class="type">uint32_t</span> m, <span class="type">uint32_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                    <span class="type">uint32_t</span> k, <span class="type">bool</span> is_A_transpose, <span class="type">bool</span> is_B_transpose,</span></span></span><br><span class="line"><span class="params"><span class="function">                    cudaStream_t stream)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Assume there is no padding in our data.</span></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> lda&#123;is_A_transpose ? k : m&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> ldb&#123;is_B_transpose ? n : k&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> ldc&#123;m&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> alpha&#123;<span class="number">1.0f</span>&#125;;</span><br><span class="line">    <span class="type">float</span> <span class="type">const</span> beta&#123;<span class="number">0.0f</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> WMMA_M&#123;<span class="number">16</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> WMMA_N&#123;<span class="number">16</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> WMMA_K&#123;<span class="number">16</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> WARP_SIZE&#123;<span class="number">32</span>&#125;;</span><br><span class="line"></span><br><span class="line">    dim3 gridDim;</span><br><span class="line">    dim3 blockDim;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// blockDim.x must be a multple of warpSize</span></span><br><span class="line">    <span class="comment">// Block size of 128x4 means we have 16 (4x4) warps,</span></span><br><span class="line">    <span class="comment">// each warp computes a 16x16 output tile,</span></span><br><span class="line">    <span class="comment">// and a block computes a 64x64 output tile.</span></span><br><span class="line">    <span class="comment">// Each block has 4x4 warps, totalling 4x4x32 threads.</span></span><br><span class="line">    <span class="type">int</span> <span class="type">const</span> num_warps_x = <span class="number">4</span>;</span><br><span class="line">    <span class="type">int</span> <span class="type">const</span> num_warps_y = <span class="number">4</span>;</span><br><span class="line">    blockDim.x = num_warps_x * WARP_SIZE;</span><br><span class="line">    blockDim.y = num_warps_y;</span><br><span class="line">    <span class="comment">// Round up.</span></span><br><span class="line">    gridDim.x = (m + (WMMA_M * num_warps_x - <span class="number">1</span>)) / (WMMA_M * num_warps_x);</span><br><span class="line">    gridDim.y = (n + WMMA_N * num_warps_y - <span class="number">1</span>) / (WMMA_N * num_warps_y);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// C = A * B</span></span><br><span class="line">    <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        wmma_gemm_a_col_major_b_col_major&lt;T1, T2, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                                          nvcuda::wmma::col_major,</span><br><span class="line">                                          nvcuda::wmma::col_major&gt;</span><br><span class="line">            &lt;&lt;&lt;gridDim, blockDim, <span class="number">0</span>, stream&gt;&gt;&gt;(A, B, C, m, n, k, lda, ldb, ldc,</span><br><span class="line">                                               is_A_transpose, is_B_transpose,</span><br><span class="line">                                               alpha, beta);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A^T * B</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ((is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        wmma_gemm_a_col_major_b_col_major&lt;T1, T2, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                                          nvcuda::wmma::row_major,</span><br><span class="line">                                          nvcuda::wmma::col_major&gt;</span><br><span class="line">            &lt;&lt;&lt;gridDim, blockDim, <span class="number">0</span>, stream&gt;&gt;&gt;(A, B, C, m, n, k, lda, ldb, ldc,</span><br><span class="line">                                               is_A_transpose, is_B_transpose,</span><br><span class="line">                                               alpha, beta);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A * B^T</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        wmma_gemm_a_col_major_b_col_major&lt;T1, T2, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                                          nvcuda::wmma::col_major,</span><br><span class="line">                                          nvcuda::wmma::row_major&gt;</span><br><span class="line">            &lt;&lt;&lt;gridDim, blockDim, <span class="number">0</span>, stream&gt;&gt;&gt;(A, B, C, m, n, k, lda, ldb, ldc,</span><br><span class="line">                                               is_A_transpose, is_B_transpose,</span><br><span class="line">                                               alpha, beta);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A^T * B^T</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        wmma_gemm_a_col_major_b_col_major&lt;T1, T2, WMMA_M, WMMA_N, WMMA_K,</span><br><span class="line">                                          nvcuda::wmma::row_major,</span><br><span class="line">                                          nvcuda::wmma::row_major&gt;</span><br><span class="line">            &lt;&lt;&lt;gridDim, blockDim, <span class="number">0</span>, stream&gt;&gt;&gt;(A, B, C, m, n, k, lda, ldb, ldc,</span><br><span class="line">                                               is_A_transpose, is_B_transpose,</span><br><span class="line">                                               alpha, beta);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">CHECK_LAST_CUDA_ERROR</span>();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// A and B are column-major matrices.</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">mm_a_col_major_b_col_major</span><span class="params">(T1 <span class="type">const</span>* A, T1 <span class="type">const</span>* B, T2* C, <span class="type">uint32_t</span> m,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">uint32_t</span> n, <span class="type">uint32_t</span> k, <span class="type">uint32_t</span> lda,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">uint32_t</span> ldb, <span class="type">uint32_t</span> ldc, <span class="type">bool</span> is_A_transpose,</span></span></span><br><span class="line"><span class="params"><span class="function">                                <span class="type">bool</span> is_B_transpose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">uint32_t</span> ni&#123;<span class="number">0</span>&#125;; ni &lt; n; ++ni)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">uint32_t</span> mi&#123;<span class="number">0</span>&#125;; mi &lt; m; ++mi)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="comment">// Compute C[mi, ni]</span></span><br><span class="line">            T2 accum&#123;<span class="number">0</span>&#125;;</span><br><span class="line">            <span class="comment">// C = A * B</span></span><br><span class="line">            <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[mi, ki] * B[ki, ni]</span></span><br><span class="line">                    accum += A[ki * lda + mi] * B[ni * ldb + ki];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// C = A^T * B</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[ki, mi] * B[ki, ni]</span></span><br><span class="line">                    accum += A[mi * lda + ki] * B[ni * ldb + ki];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// C = A * B^T</span></span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (is_B_transpose))</span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[mi, ki] * B[ni, ki]</span></span><br><span class="line">                    accum += A[ki * lda + mi] * B[ki * ldb + ni];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// C = A^T * B^T</span></span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            &#123;</span><br><span class="line">                <span class="keyword">for</span> (<span class="type">uint32_t</span> ki&#123;<span class="number">0</span>&#125;; ki &lt; k; ++ki)</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="comment">// A[ki, mi] * B[ni, ki]</span></span><br><span class="line">                    accum += A[mi * lda + ki] * B[ki * ldb + ni];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            C[ni * ldc + mi] = accum;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T1, <span class="keyword">typename</span> T2&gt;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">launch_mm</span><span class="params">(T1 <span class="type">const</span>* A, T1 <span class="type">const</span>* B, T2* C, <span class="type">uint32_t</span> m, <span class="type">uint32_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">               <span class="type">uint32_t</span> k, <span class="type">bool</span> is_A_transpose, <span class="type">bool</span> is_B_transpose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// Assume there is no padding in our data.</span></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> lda&#123;is_A_transpose ? k : m&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> ldb&#123;is_B_transpose ? n : k&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> ldc&#123;m&#125;;</span><br><span class="line">    <span class="built_in">mm_a_col_major_b_col_major</span>(A, B, C, m, n, k, lda, ldb, ldc, is_A_transpose,</span><br><span class="line">                               is_B_transpose);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fill_random_float_values</span><span class="params">(<span class="type">float</span>* arr, <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                              std::default_random_engine&amp; e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::uniform_real_distribution&lt;<span class="type">float</span>&gt; <span class="title">uniform_dist</span><span class="params">(<span class="number">-256</span>, <span class="number">256</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        arr[i] = <span class="built_in">uniform_dist</span>(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fill_random_int8_values</span><span class="params">(<span class="type">int8_t</span>* arr, <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                             std::default_random_engine&amp; e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::uniform_int_distribution&lt;<span class="type">int8_t</span>&gt; <span class="title">uniform_dist</span><span class="params">(<span class="number">-128</span>, <span class="number">127</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        arr[i] = <span class="built_in">uniform_dist</span>(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">fill_random_int32_values</span><span class="params">(<span class="type">int32_t</span>* arr, <span class="type">size_t</span> n,</span></span></span><br><span class="line"><span class="params"><span class="function">                              std::default_random_engine&amp; e)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="function">std::uniform_int_distribution&lt;<span class="type">int32_t</span>&gt; <span class="title">uniform_dist</span><span class="params">(<span class="number">-128</span>, <span class="number">127</span>)</span></span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        arr[i] = <span class="built_in">uniform_dist</span>(e);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">float2half</span><span class="params">(__half* half_arr, <span class="type">float</span> <span class="type">const</span>* float_arr, <span class="type">size_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        half_arr[i] = __float2half(float_arr[i]);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">get_avg_abs_diff_ratio</span><span class="params">(T <span class="type">const</span>* arr_1, T <span class="type">const</span>* arr_2, <span class="type">size_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> sum_abs_diff_ratio&#123;<span class="number">0</span>&#125;;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        sum_abs_diff_ratio += std::<span class="built_in">abs</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(arr_1[i]) -</span><br><span class="line">                                       <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(arr_2[i])) /</span><br><span class="line">                              std::<span class="built_in">abs</span>(<span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(arr_1[i]) +</span><br><span class="line">                                       <span class="built_in">static_cast</span>&lt;<span class="type">float</span>&gt;(arr_2[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> sum_abs_diff_ratio / n;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="type">bool</span> <span class="title">array_equal</span><span class="params">(T <span class="type">const</span>* arr_1, T <span class="type">const</span>* arr_2, <span class="type">size_t</span> n)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">size_t</span> i&#123;<span class="number">0</span>&#125;; i &lt; n; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr_1[i] != arr_2[i])</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">false</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">print_test_header</span><span class="params">(<span class="type">bool</span> is_A_transpose, <span class="type">bool</span> is_B_transpose)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// C = A * B</span></span><br><span class="line">    <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;C = A * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A^T * B</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ((is_A_transpose) &amp;&amp; (!is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;C = A^T * B&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A * B^T</span></span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">if</span> ((!is_A_transpose) &amp;&amp; (is_B_transpose))</span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;C = A * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// C = A^T * B^T</span></span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;C = A^T * B^T&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> num_repeats&#123;<span class="number">10</span>&#125;;</span><br><span class="line">    <span class="keyword">constexpr</span> <span class="type">int</span> num_warmups&#123;<span class="number">10</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_size_m&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_size_n&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    <span class="type">uint32_t</span> <span class="type">const</span> matrix_size_k&#123;<span class="number">1024</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Matrix Sizes&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;M: &quot;</span> &lt;&lt; matrix_size_m &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;N: &quot;</span> &lt;&lt; matrix_size_n &lt;&lt; std::endl;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;K: &quot;</span> &lt;&lt; matrix_size_k &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="function">std::default_random_engine <span class="title">random_engine</span><span class="params">(<span class="number">0</span>)</span></span>;</span><br><span class="line"></span><br><span class="line">    cudaStream_t stream;</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamCreate</span>(&amp;stream));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// HMMA</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;FP16 HMMA&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_a_float</span><span class="params">(matrix_size_m * matrix_size_k)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_b_float</span><span class="params">(matrix_size_k * matrix_size_n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;__half&gt; <span class="title">matrix_a_half</span><span class="params">(matrix_size_m * matrix_size_k)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;__half&gt; <span class="title">matrix_b_half</span><span class="params">(matrix_size_k * matrix_size_n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_c_float</span><span class="params">(matrix_size_m * matrix_size_n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">float</span>&gt; <span class="title">matrix_c_float_reference</span><span class="params">(matrix_size_m * matrix_size_n)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span>* h_matrix_a_float&#123;matrix_a_float.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">float</span>* h_matrix_b_float&#123;matrix_b_float.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    __half* h_matrix_a_half&#123;matrix_a_half.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    __half* h_matrix_b_half&#123;matrix_b_half.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">float</span>* h_matrix_c_float&#123;matrix_c_float.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">float</span>* h_matrix_c_float_reference&#123;matrix_c_float_reference.<span class="built_in">data</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">fill_random_float_values</span>(h_matrix_a_float, matrix_a_float.<span class="built_in">size</span>(),</span><br><span class="line">                             random_engine);</span><br><span class="line">    <span class="built_in">fill_random_float_values</span>(h_matrix_b_float, matrix_b_float.<span class="built_in">size</span>(),</span><br><span class="line">                             random_engine);</span><br><span class="line">    <span class="built_in">fill_random_float_values</span>(h_matrix_c_float, matrix_c_float.<span class="built_in">size</span>(),</span><br><span class="line">                             random_engine);</span><br><span class="line">    <span class="built_in">fill_random_float_values</span>(h_matrix_c_float_reference,</span><br><span class="line">                             matrix_c_float_reference.<span class="built_in">size</span>(), random_engine);</span><br><span class="line">    <span class="built_in">float2half</span>(h_matrix_a_half, h_matrix_a_float, matrix_a_float.<span class="built_in">size</span>());</span><br><span class="line">    <span class="built_in">float2half</span>(h_matrix_b_half, h_matrix_b_float, matrix_b_float.<span class="built_in">size</span>());</span><br><span class="line"></span><br><span class="line">    half *d_matrix_a_half, *d_matrix_b_half;</span><br><span class="line">    <span class="type">float</span>* d_matrix_c_float;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;d_matrix_a_half,</span><br><span class="line">                                matrix_size_m * matrix_size_k * <span class="built_in">sizeof</span>(half)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;d_matrix_b_half,</span><br><span class="line">                                matrix_size_k * matrix_size_n * <span class="built_in">sizeof</span>(half)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(&amp;d_matrix_c_float,</span><br><span class="line">                                matrix_size_m * matrix_size_n * <span class="built_in">sizeof</span>(<span class="type">float</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Copy data from host to device.</span></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(d_matrix_a_half, h_matrix_a_half,</span><br><span class="line">                                matrix_a_float.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(__half),</span><br><span class="line">                                cudaMemcpyHostToDevice));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(d_matrix_b_half, h_matrix_b_half,</span><br><span class="line">                                matrix_b_float.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(__half),</span><br><span class="line">                                cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">bool</span> is_A_transpose : &#123;<span class="literal">true</span>, <span class="literal">false</span>&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">bool</span> is_B_transpose : &#123;<span class="literal">true</span>, <span class="literal">false</span>&#125;)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">print_test_header</span>(is_A_transpose, is_B_transpose);</span><br><span class="line">            <span class="comment">// Compute matrix multiplication reference output using CPU.</span></span><br><span class="line">            <span class="built_in">launch_mm</span>(h_matrix_a_float, h_matrix_b_float,</span><br><span class="line">                      h_matrix_c_float_reference, matrix_size_m, matrix_size_n,</span><br><span class="line">                      matrix_size_k, is_A_transpose, is_B_transpose);</span><br><span class="line">            <span class="comment">// Compute matrix multiplication reference output using CUDA WMMA.</span></span><br><span class="line">            <span class="built_in">launch_wmma_mm</span>(d_matrix_a_half, d_matrix_b_half, d_matrix_c_float,</span><br><span class="line">                           matrix_size_m, matrix_size_n, matrix_size_k,</span><br><span class="line">                           is_A_transpose, is_B_transpose, stream);</span><br><span class="line">            <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line"></span><br><span class="line">            <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(h_matrix_c_float, d_matrix_c_float,</span><br><span class="line">                                        matrix_c_float.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">float</span>),</span><br><span class="line">                                        cudaMemcpyDeviceToHost));</span><br><span class="line"></span><br><span class="line">            <span class="type">float</span> <span class="type">const</span> avg_abs_diff_ratio&#123;<span class="built_in">get_avg_abs_diff_ratio</span>(</span><br><span class="line">                h_matrix_c_float, h_matrix_c_float_reference,</span><br><span class="line">                matrix_c_float.<span class="built_in">size</span>())&#125;;</span><br><span class="line">            <span class="keyword">if</span> (avg_abs_diff_ratio &gt; <span class="number">0.01</span>)</span><br><span class="line">            &#123;</span><br><span class="line">                std::cout &lt;&lt; <span class="string">&quot;Got high average absolute diff ratio: &quot;</span></span><br><span class="line">                          &lt;&lt; avg_abs_diff_ratio &lt;&lt; std::endl;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Performance measurement.</span></span><br><span class="line">            std::function&lt;<span class="type">void</span>(cudaStream_t)&gt; <span class="type">const</span> function_hmma&#123;std::<span class="built_in">bind</span>(</span><br><span class="line">                launch_wmma_mm&lt;__half, <span class="type">float</span>&gt;, d_matrix_a_half, d_matrix_b_half,</span><br><span class="line">                d_matrix_c_float, matrix_size_m, matrix_size_n, matrix_size_k,</span><br><span class="line">                is_A_transpose, is_B_transpose, std::placeholders::_1)&#125;;</span><br><span class="line">            <span class="type">float</span> <span class="type">const</span> latency_hmma&#123;<span class="built_in">measure_performance</span>(</span><br><span class="line">                function_hmma, stream, num_repeats, num_warmups)&#125;;</span><br><span class="line">            std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">3</span>)</span><br><span class="line">                      &lt;&lt; <span class="string">&quot;HMMA Latency: &quot;</span> &lt;&lt; latency_hmma &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_a_half));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_b_half));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_c_float));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// IMMA</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;INT8 IMMA&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int8_t</span>&gt; <span class="title">matrix_a_int8</span><span class="params">(matrix_size_m * matrix_size_k)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int8_t</span>&gt; <span class="title">matrix_b_int8</span><span class="params">(matrix_size_k * matrix_size_n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int32_t</span>&gt; <span class="title">matrix_c_int32</span><span class="params">(matrix_size_m * matrix_size_n)</span></span>;</span><br><span class="line">    <span class="function">std::vector&lt;<span class="type">int32_t</span>&gt; <span class="title">matrix_c_int32_reference</span><span class="params">(matrix_size_m *</span></span></span><br><span class="line"><span class="params"><span class="function">                                                  matrix_size_n)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">int8_t</span>* h_matrix_a_int8&#123;matrix_a_int8.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int8_t</span>* h_matrix_b_int8&#123;matrix_b_int8.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int32_t</span>* h_matrix_c_int32&#123;matrix_c_int32.<span class="built_in">data</span>()&#125;;</span><br><span class="line">    <span class="type">int32_t</span>* h_matrix_c_int32_reference&#123;matrix_c_int32_reference.<span class="built_in">data</span>()&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">fill_random_int8_values</span>(h_matrix_a_int8, matrix_a_int8.<span class="built_in">size</span>(),</span><br><span class="line">                            random_engine);</span><br><span class="line">    <span class="built_in">fill_random_int8_values</span>(h_matrix_b_int8, matrix_b_int8.<span class="built_in">size</span>(),</span><br><span class="line">                            random_engine);</span><br><span class="line">    <span class="built_in">fill_random_int32_values</span>(h_matrix_c_int32, matrix_c_int32.<span class="built_in">size</span>(),</span><br><span class="line">                             random_engine);</span><br><span class="line">    <span class="built_in">fill_random_int32_values</span>(h_matrix_c_int32_reference,</span><br><span class="line">                             matrix_c_int32_reference.<span class="built_in">size</span>(), random_engine);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Profile INT8 IMMA without verifying the correctness.</span></span><br><span class="line">    <span class="type">int8_t</span> *d_matrix_a_int8, *d_matrix_b_int8;</span><br><span class="line">    <span class="type">int32_t</span>* d_matrix_c_int32;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(</span><br><span class="line">        &amp;d_matrix_a_int8, matrix_size_m * matrix_size_k * <span class="built_in">sizeof</span>(<span class="type">int8_t</span>)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(</span><br><span class="line">        &amp;d_matrix_b_int8, matrix_size_k * matrix_size_n * <span class="built_in">sizeof</span>(<span class="type">int8_t</span>)));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMalloc</span>(</span><br><span class="line">        &amp;d_matrix_c_int32, matrix_size_m * matrix_size_n * <span class="built_in">sizeof</span>(<span class="type">int32_t</span>)));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(d_matrix_a_int8, h_matrix_a_int8,</span><br><span class="line">                                matrix_a_int8.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">int8_t</span>),</span><br><span class="line">                                cudaMemcpyHostToDevice));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(d_matrix_b_int8, h_matrix_b_int8,</span><br><span class="line">                                matrix_b_int8.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">int8_t</span>),</span><br><span class="line">                                cudaMemcpyHostToDevice));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (<span class="type">bool</span> is_A_transpose : &#123;<span class="literal">true</span>, <span class="literal">false</span>&#125;)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">bool</span> is_B_transpose : &#123;<span class="literal">true</span>, <span class="literal">false</span>&#125;)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="built_in">print_test_header</span>(is_A_transpose, is_B_transpose);</span><br><span class="line">            <span class="comment">// Compute matrix multiplication reference output using CPU.</span></span><br><span class="line">            <span class="built_in">launch_mm</span>(h_matrix_a_int8, h_matrix_b_int8,</span><br><span class="line">                      h_matrix_c_int32_reference, matrix_size_m, matrix_size_n,</span><br><span class="line">                      matrix_size_k, is_A_transpose, is_B_transpose);</span><br><span class="line">            <span class="comment">// Compute matrix multiplication reference output using CUDA WMMA.</span></span><br><span class="line">            <span class="built_in">launch_wmma_mm</span>(d_matrix_a_int8, d_matrix_b_int8, d_matrix_c_int32,</span><br><span class="line">                           matrix_size_m, matrix_size_n, matrix_size_k,</span><br><span class="line">                           is_A_transpose, is_B_transpose, stream);</span><br><span class="line">            <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamSynchronize</span>(stream));</span><br><span class="line">            <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaMemcpy</span>(h_matrix_c_int32, d_matrix_c_int32,</span><br><span class="line">                                        matrix_c_int32.<span class="built_in">size</span>() * <span class="built_in">sizeof</span>(<span class="type">int32_t</span>),</span><br><span class="line">                                        cudaMemcpyDeviceToHost));</span><br><span class="line">            <span class="comment">// Integer matrix multiplications from CPU and CUDA should be</span></span><br><span class="line">            <span class="comment">// bitwise identical.</span></span><br><span class="line">            <span class="built_in">assert</span>(<span class="built_in">array_equal</span>(h_matrix_c_int32, h_matrix_c_int32_reference,</span><br><span class="line">                               matrix_c_int32.<span class="built_in">size</span>()));</span><br><span class="line"></span><br><span class="line">            <span class="comment">// Performance measurement.</span></span><br><span class="line">            std::function&lt;<span class="type">void</span>(cudaStream_t)&gt; <span class="type">const</span> function_imma&#123;</span><br><span class="line">                std::<span class="built_in">bind</span>(launch_wmma_mm&lt;<span class="type">int8_t</span>, <span class="type">int32_t</span>&gt;, d_matrix_a_int8,</span><br><span class="line">                          d_matrix_b_int8, d_matrix_c_int32, matrix_size_m,</span><br><span class="line">                          matrix_size_n, matrix_size_k, is_A_transpose,</span><br><span class="line">                          is_B_transpose, std::placeholders::_1)&#125;;</span><br><span class="line">            <span class="type">float</span> <span class="type">const</span> latency_imma&#123;<span class="built_in">measure_performance</span>(</span><br><span class="line">                function_imma, stream, num_repeats, num_warmups)&#125;;</span><br><span class="line">            std::cout &lt;&lt; std::fixed &lt;&lt; std::<span class="built_in">setprecision</span>(<span class="number">3</span>)</span><br><span class="line">                      &lt;&lt; <span class="string">&quot;IMMA Latency: &quot;</span> &lt;&lt; latency_imma &lt;&lt; <span class="string">&quot; ms&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_a_int8));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_b_int8));</span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaFree</span>(d_matrix_c_int32));</span><br><span class="line"></span><br><span class="line">    <span class="built_in">CHECK_CUDA_ERROR</span>(<span class="built_in">cudaStreamDestroy</span>(stream));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>All the transposed matrix multiplication implementations did not actually transpose the matrices. Instead, we used the row-major and column-major trick introduced in my previous article <a href="/blog/Row-Major-VS-Column-Major/">“Row-Major VS Column-Major”</a>.</p>
<p>We also observed that for matrix multiplication for matrices stored in column-major order, $C = A^{\top}B$ is the fastest and $C = A B^{\top}$ is the slowest, for GEMM implementations using HMMA and IMMA instructions on an NVIDIA RTX 3090 GPU.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ nvcc mma.cu -o mma --gpu-architecture=compute_86</span><br><span class="line">$ ./mma</span><br><span class="line">Matrix Sizes</span><br><span class="line">M: 1024</span><br><span class="line">N: 1024</span><br><span class="line">K: 1024</span><br><span class="line">FP16 HMMA</span><br><span class="line">C = A^T * B^T</span><br><span class="line">HMMA Latency: 0.177 ms</span><br><span class="line">C = A^T * B</span><br><span class="line">HMMA Latency: 0.169 ms</span><br><span class="line">C = A * B^T</span><br><span class="line">HMMA Latency: 0.189 ms</span><br><span class="line">C = A * B</span><br><span class="line">HMMA Latency: 0.177 ms</span><br><span class="line">INT8 IMMA</span><br><span class="line">C = A^T * B^T</span><br><span class="line">IMMA Latency: 0.129 ms</span><br><span class="line">C = A^T * B</span><br><span class="line">IMMA Latency: 0.090 ms</span><br><span class="line">C = A * B^T</span><br><span class="line">IMMA Latency: 0.170 ms</span><br><span class="line">C = A * B</span><br><span class="line">IMMA Latency: 0.129 ms</span><br></pre></td></tr></table></figure>

<h2 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h2><p>NVIDIA Tensor Cores are programmable and can be used for accelerating computations that are dominated by GEMM operations.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://developer.nvidia.com/blog/programming-tensor-cores-cuda-9/">Programming Tensor Cores in CUDA 9</a></li>
<li><a target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/archive/12.0.1/cuda-c-programming-guide/#warp-matrix-functions">CUDA C Programming Guide - Warp Matrix Functions</a></li>
<li><a href="/blog/CUDA-Matrix-Multiplication/">CUDA Matrix Multiplication</a></li>
<li><a href="/blog/Row-Major-VS-Column-Major/">Row-Major VS Column-Major</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>NVIDIA Tensor Core Programming</p><p><a href="https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/">https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>05-18-2023</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>12-27-2023</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/Accelerated-Computing/">Accelerated Computing,</a> </div><div><a class="link-muted" rel="tag" href="/tags/CUDA/">CUDA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/NVIDIA/">NVIDIA,</a> </div><div><a class="link-muted" rel="tag" href="/tags/C/">C++ </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/leimao" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/life/Sycamore-Grove-Park/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">Sycamore Grove Park 徒步</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/essay/%E6%B0%B4%E6%B5%92%E4%BC%A0%E4%B8%80%E7%99%BE%E9%9B%B6%E5%85%AB%E5%B0%86%E5%90%8D%E5%AD%97%E5%9C%A8%E5%8E%9F%E8%91%97%E4%B8%AD%E7%9A%84%E5%87%BA%E7%8E%B0%E9%A2%91%E7%8E%87/"><span class="level-item">《水浒传》一百零八将名字在原著中的出现频率</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://levanliu.github.io/blog/NVIDIA-Tensor-Core-Programming/';
            this.page.identifier = '/blog/NVIDIA-Tensor-Core-Programming/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'leimao-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/leimao" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/leimao" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchaleimao"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukeleimao@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#NVIDIA-Tensor-Core"><span class="level-left"><span class="level-item">2</span><span class="level-item">NVIDIA Tensor Core</span></span></a></li><li><a class="level is-mobile" href="#NVIDIA-Tensor-Core-Programming"><span class="level-left"><span class="level-item">3</span><span class="level-item">NVIDIA Tensor Core Programming</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Matrix-Multiplication-Decomposition"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Matrix Multiplication Decomposition</span></span></a></li><li><a class="level is-mobile" href="#Matrix-Multiplication-Implementation-Using-NVIDIA-Tensor-Core"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Matrix Multiplication Implementation Using NVIDIA Tensor Core</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Conclusions"><span class="level-left"><span class="level-item">4</span><span class="level-item">Conclusions</span></span></a></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">5</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=leimaogithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>