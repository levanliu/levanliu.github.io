<!doctype html>
<html lang="en"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="twitter:image:src" content="https://levanliu.github.io/images/favicon/android-chrome-512x512.png"><meta name="referrer" content="no-referrer-when-downgrade"><title>Deformable Attention - Lei Mao&#039;s Log Book</title><link rel="manifest" href="/manifest.json"><meta name="theme-color" content="#3273dc"><meta name="application-name" content="Lei Mao&#039;s Log Book"><meta name="msapplication-TileImage" content="/images/favicon/android-chrome-192x192.png"><meta name="msapplication-TileColor" content="#3273dc"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Lei Mao&#039;s Log Book"><meta name="apple-mobile-web-app-status-bar-style" content="default"><link rel="apple-touch-icon" sizes="192x192" href="/images/favicon/android-chrome-192x192.png"><link rel="apple-touch-icon" sizes="512x512" href="/images/favicon/android-chrome-512x512.png"><link rel="apple-touch-icon" sizes="180x180" href="/images/favicon/apple-touch-icon.png"><link rel="apple-touch-icon" sizes="16x16" href="/images/favicon/favicon-16x16.png"><link rel="apple-touch-icon" sizes="32x32" href="/images/favicon/favicon-32x32.png"><meta name="description" content="Attention with Learned Spatial Feature Sampling"><meta property="og:type" content="blog"><meta property="og:title" content="Deformable Attention"><meta property="og:url" content="https://levanliu.github.io/blog/Deformable-Attention/"><meta property="og:site_name" content="Lei Mao&#039;s Log Book"><meta property="og:description" content="Attention with Learned Spatial Feature Sampling"><meta property="og:locale" content="en_US"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_attention.png"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_detr.png"><meta property="og:image" content="https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_attention_v2.png"><meta property="article:published_time" content="2023-12-16T08:00:00.000Z"><meta property="article:modified_time" content="2023-12-16T08:00:00.000Z"><meta property="article:author" content="Lei Mao"><meta property="article:tag" content="Deep Learning"><meta property="article:tag" content="Computer Vision"><meta property="article:tag" content="Transformer"><meta property="article:tag" content="Attention"><meta property="twitter:card" content="summary"><meta property="twitter:image:src" content="https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_attention.png"><meta property="twitter:creator" content="@matchaleimao"><meta property="twitter:site" content="Lei Mao&#039;s Log Book"><meta property="fb:admins" content="dukeleimao"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://levanliu.github.io/blog/Deformable-Attention/"},"headline":"Deformable Attention","image":["https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_attention.png","https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_detr.png","https://levanliu.github.io/images/blog/2023-12-16-Deformable-Attention/deformable_attention_v2.png"],"datePublished":"2023-12-16T08:00:00.000Z","dateModified":"2023-12-16T08:00:00.000Z","author":{"@type":"Person","name":"Lei Mao"},"publisher":{"@type":"Organization","name":"Lei Mao's Log Book","logo":{"@type":"ImageObject","url":"https://levanliu.github.io/images/favicon/android-chrome-512x512.png"}},"description":"Attention with Learned Spatial Feature Sampling"}</script><link rel="canonical" href="https://levanliu.github.io/blog/Deformable-Attention/"><link rel="alternate" href="/atom.xml" title="Lei Mao&#039;s Log Book" type="application/atom+xml"><link rel="icon" href="/images/favicon/favicon.ico"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v6.0.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=PT+Sans+Narrow:wght@400;700&amp;family=PT+Serif"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><!--!--><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/css/justifiedGallery.min.css"><script src="https://www.googletagmanager.com/gtag/js?id=G-EJY6FXZBCB" async></script><script>window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
    
        gtag('config', 'G-EJY6FXZBCB');</script><!--!--><!--!--><style>.pace{-webkit-pointer-events:none;pointer-events:none;-webkit-user-select:none;-moz-user-select:none;user-select:none}.pace-inactive{display:none}.pace .pace-progress{background:#3273dc;position:fixed;z-index:2000;top:0;right:100%;width:100%;height:2px}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js"></script><!--!--><!--!--><script src="//m.servedby-buysellads.com/monetization.custom.js"></script><meta name="generator" content="Hexo 5.4.0"></head><body class="is-3-column"><nav class="navbar navbar-main"><div class="container navbar-container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">Lei Mao&#039;s Log Book</a><a class="navbar-item" href="/curriculum">Curriculum</a><a class="navbar-item" href="/blog">Blog</a><a class="navbar-item" href="/article">Articles</a><a class="navbar-item" href="/project">Projects</a><a class="navbar-item" href="/publication">Publications</a><a class="navbar-item" href="/reading">Readings</a><a class="navbar-item" href="/life">Life</a><a class="navbar-item" href="/essay">Essay</a><a class="navbar-item" href="/archives">Archives</a><a class="navbar-item" href="/categories">Categories</a><a class="navbar-item" href="/tags">Tags</a><a class="navbar-item" href="/faq">FAQs</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Switch Color Scheme" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="Gitter" href="https://gitter.im/leimao/community"><i class="fab fa-gitter"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="Catalogue" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="Search" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-content"><center><div class="bsa-standard" id="carbon-ad-01"></div><script>            (function() {             if (typeof _bsa !== 'undefined' && _bsa) {                 _bsa.init('custom', 'CWYD65QY', 'placement:leimaogithubio-standard', {                     target: '#carbon-ad-01',                     template: `             <a href='##link##' class='native-banner' style='background: ##backgroundColor##' rel='sponsored noopener' target='_blank' title='##company## — ##tagline##'>                 <img class='native-img' width='125' src='##logo##' />                 <div class='native-main'>                     <div class='native-details' style='                             color: ##textColor##;                             border-left: solid 1px ##textColor##;                         '>                         <span class='native-company'>Sponsored by ##company##</span>                         <span class='native-desc'>##description##</span>                     </div>                     <span class='native-cta' style='                             color: ##ctaTextColor##;                             background-color: ##ctaBackgroundColor##;                         '>##callToAction##</span>                 </div>             </a>             `,                 });                 }             })();         </script></center></div></div><div class="card"><article class="card-content article" role="article"><h1 class="title is-size-3" style="font-family: 'PT Sans Narrow', sans-serif">Deformable Attention</h1><div class="article-meta is-size-7 is-uppercase level is-mobile"><div class="level-left" style="margin-bottom: 0.50rem"><span class="level-item"><i class="far fa-calendar-alt"> </i><time dateTime="2023-12-16T08:00:00.000Z" title="2023-12-16T08:00:00.000Z">12-16-2023</time></span><span class="level-item is-hidden-mobile"><i class="far fa-calendar-check"> </i><time dateTime="2023-12-16T08:00:00.000Z" title="2023-12-16T08:00:00.000Z">12-16-2023</time></span><span class="level-item"><i class="far fa-folder-open has-text-grey"></i> <a class="link-muted" href="/blog/">blog</a></span><span class="level-item"><i class="far fa-clock"></i> 34 minutes read (About 5042 words)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;<span id="busuanzi_value_page_pv"><i class="fa fa-spinner fa-spin"></i></span>&nbsp;visits</span></div></div><div class="content" style="margin-top: 1.0rem"><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Transformer multi-head attention is a mechanism that allows a neural network to focus on a small set of features from a large set of features. However, the vanilla multi-head attention mechanism is usually very expensive to compute and slows down training convergence, especially for computer vision models. Deformable multi-head attention was developed to reduce the computational complexity of the attention mechanism for computer vision models.</p>
<p>In this blog post, I would like to discuss the vanilla multi-head attention, the deformable multi-head attention, and the deformable multi-head attention v2 in detail.</p>
<h2 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h2><h3 id="Multi-Head-Attention-Formulation"><a href="#Multi-Head-Attention-Formulation" class="headerlink" title="Multi-Head Attention Formulation"></a>Multi-Head Attention Formulation</h3><p>Let $q \in \Omega_q$ indexes a query element with representation feature $\mathbf{z}_q \in \mathbb{R}^{C \times 1}$, and $k \in \Omega_k$ indexes a key element with representation feature $\mathbf{x}_k \in \mathbb{R}^{C \times 1}$, where $C$ is the feature dimension, and $\Omega_q$ and $\Omega_k$ are the query and key element sets, respectively. The multi-head attention feature is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{MultiHeadAttention}(\mathbf{z}_q, \mathbf{X}) &amp;= \sum_{m = 1}^{M} \mathbf{W}_m \left[ \sum_{k \in \Omega_k}^{} A_{m,q,k} \mathbf{W}_m^{\prime} \mathbf{x}_{k} \right] \\<br>\end{aligned}<br>$$</p>
<p>where $m$ indexes the attention head, $M$ is the number of attention heads, $\mathbf{W}_m \in \mathbb{R}^{C \times C_{v}}$ and $\mathbf{W}_m^{\prime} \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, $C_{v} = \frac{C}{M}$ by default, and $A_{m,q,k} \in \mathbb{R}$ is the attention weight between the query element $q$ and the key element $k$ for the $m$-th attention head.</p>
<p>The attention weight $A_{m,q,k}$, which is normalized, is computed as follows using a softmax function:</p>
<p>$$<br>\begin{aligned}<br>A_{m,q,k} &amp;= \frac{\exp \left( \text{score}(\mathbf{z}_q, \mathbf{x}_k) \right)}{\sum_{k^{\prime} \in \Omega_k}^{} \exp \left( \text{score}(\mathbf{z}_q, \mathbf{x}_{k^{\prime}}) \right)} \\<br>&amp;= \frac{\exp \left( \frac{\left( \mathbf{U}_m \mathbf{z}_q \right)^{\top} \mathbf{V}_m \mathbf{x}_k}{\sqrt{C_v}} \right)}{\sum_{k^{\prime} \in \Omega_k}^{} \exp \left( \frac{\left( \mathbf{U}_m \mathbf{z}_q \right)^{\top} \mathbf{V}_m \mathbf{x}_{k^{\prime}}}{\sqrt{C_v}} \right)} \\<br>&amp;= \frac{\exp \left( \frac{ \mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_k }{\sqrt{C_v}} \right)}{\sum_{k^{\prime} \in \Omega_k}^{} \exp \left( \frac{\mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_{k^{\prime}}}{\sqrt{C_v}} \right)} \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{U}_m \in \mathbb{R}^{C_{v} \times C}$ and $\mathbf{V}_m \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, and $\text{score}(\mathbf{z}_q, \mathbf{x}_k) = \frac{ \mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_k }{\sqrt{C_v}}$ is the attention score between the query element $q$ and the key element $k$ for the $m$-th attention head.</p>
<p>$\sum_{k \in \Omega_k}^{} A_{m,q,k} \mathbf{W}_m^{\prime} \mathbf{x}_{k}$ for all the queries can be computed using vectorization as follows:</p>
<p>$$<br>\begin{aligned}<br>\mathbf{A}_{m} \mathbf{X} \mathbf{W}_m^{\prime \top}<br>&amp;= \text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \left(\mathbf{X} \mathbf{V}_m^{\top} \right)^{\top}}{\sqrt{C_v}} \right) \mathbf{X} \mathbf{W}_m^{\prime \top} \\<br>&amp;= \text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}^{\top}}{\sqrt{C_v}} \right) \mathbf{X} \mathbf{W}_m^{\prime \top} \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{Z} \in \mathbb{R}^{N_q \times C}$ and $\mathbf{X} \in \mathbb{R}^{N_k \times C}$ are the query feature matrix and the key feature matrix, respectively, and $\mathbf{A}_{m} \in \mathbb{R}^{N_q \times N_k}$ is the attention weight matrix for the $m$-th attention head.</p>
<p>The multi-head attention feature for all the queries is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{MultiHeadAttention}(\mathbf{Z}, \mathbf{X})<br>&amp;= \sum_{m = 1}^{M} \mathbf{A}_{m} \mathbf{X} \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>&amp;= \sum_{m = 1}^{M} \text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}^{\top}}{\sqrt{C_v}} \right) \mathbf{X} \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>\end{aligned}<br>$$</p>
<h3 id="Multi-Head-Attention-Computational-Complexity"><a href="#Multi-Head-Attention-Computational-Complexity" class="headerlink" title="Multi-Head Attention Computational Complexity"></a>Multi-Head Attention Computational Complexity</h3><p>Suppose each multiply-accumulate (MAC) operation takes $O(1)$ time, let’s compute the computational complexity of the multi-head attention for $N_q$ queries and $N_k$ keys.</p>
<p>The computational complexity of the multi-head attention for $N_q$ queries and $N_k$ keys can be derived as follows:</p>
<p>$$<br>\begin{aligned}<br>O\left( M \left( \underbrace{O( N_k C C_{v} ) + O( N_k C_{v} C )}_{\mathbf{X} \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}} + \underbrace{O( N_k C C_{v} ) + O( N_q C C_{v} ) + O(N_q N_k C_v) + O(N_q N_k)}_{\text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}^{\top}}{\sqrt{C_v}} \right)} + O(N_q N_k C_v) \right) \right)<br>&amp;= O\left( M \left( O( N_k C C_{v} ) + O( N_q C C_{v} ) + O( N_q N_k C_v ) \right) \right) \\<br>&amp;= O( N_k C^2 ) + O( N_q C^2 ) + O( N_q N_k C ) \\<br>&amp;= O( N_k C^2 + N_q C^2 + N_q N_k C ) \\<br>\end{aligned}<br>$$</p>
<p>In modern Transformer based large language models, for both encoder and decoder, $N_q$ and $N_k$ can be very large and much larger than $C$, therefore, the computational complexity of the multi-head attention is dominated by the third term, $O( N_q N_k C )$.</p>
<p>In Transformer based computer vision models, the values of $N_q$, $N_k$, and $C$ can be quite different in the encoder and decoder, therefore affecting the asymptotic computational complexity of the multi-head attention.</p>
<p>For the encoder, because all the attention layers are self-attention, $N_q = N_k = HW$ where $H$ and $W$ are the height and width of the feature map, respectively. Because usually $HW \gg C$, the computational complexity of the multi-head attention is $O( N_q N_k C ) = O( H^2 W^2 C )$.</p>
<p>For the decoder, the number of queries $N_q$ is much smaller than the one in the encoder. In the self-attention layers, $N_q = N_k = N$, which might be comparable to $C$, the computational complexity of the multi-head attention is $O( NC^2 + N^2 C )$. In the cross-attention layers, $N_q = N$ and $N_k = HW$. Because usually $HW \gg C$ and $HW \gg N$, the computational complexity of the multi-head attention is $O( HWC^2 + NHWC )$.</p>
<p>It’s not hard to see that the computational complexity of the multi-head attention in the encoder self-attention layers and the decoder cross-attention layers are quite expensive. Deformable attention was devised to reduce the computational complexity of the multi-head attention in these layers for computer vision models.</p>
<h2 id="Deformable-Multi-Head-Attention"><a href="#Deformable-Multi-Head-Attention" class="headerlink" title="Deformable Multi-Head Attention"></a>Deformable Multi-Head Attention</h2><h3 id="Deformable-Multi-Head-Attention-Formulation"><a href="#Deformable-Multi-Head-Attention-Formulation" class="headerlink" title="Deformable Multi-Head Attention Formulation"></a>Deformable Multi-Head Attention Formulation</h3><p>Inspired by the <a href="/blog/Deformable-Convolution/">deformable convolution</a>, the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.04159">deformable multi-head attention</a> only attends to a small set of the keys sampled around a reference point in the spatial dimension for each query for computer vision models.</p>
<p>Given an input feature map $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$, let $q \in \Omega_q$ indexes a query element with representation feature $\mathbf{z}_q \in \mathbb{R}^{C}$, and $\mathbf{p}_q$ denotes the reference point for $q$, the deformable multi-head attention feature is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{DeformableMultiHeadAttention}(\mathbf{z}_q, \mathbf{p}_q, \mathbf{X}) &amp;= \sum_{m = 1}^{M} \mathbf{W}_m \left[ \sum_{k = 1}^{K} A_{m,q,k} \mathbf{W}_m^{\prime} \mathbf{x}(\mathbf{p}_q + \Delta\mathbf{p}_{m,q,k}) \right] \\<br>\end{aligned}<br>$$</p>
<p>where $m$ indexes the attention head, $M$ is the number of attention heads, $\mathbf{W}_m \in \mathbb{R}^{C \times C_{v}}$ and $\mathbf{W}_m^{\prime} \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, $C_{v} = \frac{C}{M}$ by default, $k$ indexes the sampled key element, $K$ is the number of sampled key elements, $\Delta\mathbf{p}_{m,q,k}$ is the offset for the sampled key element $k$ for the $m$-th attention head, and $A_{m,q,k} \in \mathbb{R}$ is the attention weight between the query element $q$ and the sampled key element $k$ for the $m$-th attention head.</p>
<img src="/images/blog/2023-12-16-Deformable-Attention/deformable_attention.png" class="box px-0 py-0 ml-auto mr-auto" width="960" title="Deformable Multi-Head Attention" alt="Deformable Multi-Head Attention">
<br>

<p>To effectively reduce the computational complexity of the multi-head attention, the number of sampled key elements $K$ should be much smaller than the number of all the key elements $N_k$, i.e., $K \ll N_k$. Similar to the deformable convolution, $\mathbf{x}(\mathbf{p}_q + \Delta\mathbf{p}_{m,q,k})$ can be interpolated from the neighboring pixels on the input feature map.</p>
<p>Although the offset $\Delta\mathbf{p}_{m,q,k}$ is also learned, just like the one in the deformable convolution, it is usually learned by a linear layer instead of a convolution. This might seem to be inferior because the offset prediction now only depends on the query element $q$ for each attention head $m$, and its neighbor pixels will not be considered.</p>
<p>$$<br>\begin{aligned}<br>\Delta\mathbf{p}_{m,q,k} &amp;= \mathbf{W}^{\prime\prime}_{m,k} \mathbf{z}_q \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{W}^{\prime\prime}_{m,k} \in \mathbb{R}^{2 \times C}$ is the learnable weight matrix for the $k$-th sampled key element for the $m$-th attention head.</p>
<p>The attention weight $A_{m,q,k}$ is computed quite differently from the one in the multi-head attention. Instead of computing the attention score between the query element $q$ and the key element $k$ via dot product, the attention weight only depends on the query element $q$ for each attention head $m$. The attention weight $A_{m,q,k}$ is computed as follows using a softmax function:</p>
<p>$$<br>\begin{aligned}<br>A_{m,q,k} &amp;= \frac{\exp \left( \mathbf{W}^{\prime\prime\prime}_{m,k} \mathbf{z}_q \right)}{\sum_{k^{\prime} = 1}^{K} \exp \left( \mathbf{W}^{\prime\prime\prime}_{m,k^{\prime}} \mathbf{z}_q \right)} \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{W}^{\prime\prime\prime}_{m,k} \in \mathbb{R}^{1 \times C}$ is the learnable weight vector for the $k$-th sampled key element for the $m$-th attention head.</p>
<p>The deformable multi-head attention feature for all the queries is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{DeformableMultiHeadAttention}(\mathbf{Z}, \mathbf{P}, \mathbf{X})<br>&amp;= \sum_{m = 1}^{M} \mathbf{A}_{m}  \mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right) \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>&amp;= \sum_{m = 1}^{M} \text{softmax}\left( \mathbf{Z} \mathbf{W}^{\prime\prime\prime\top}_m \right) \mathbf{X}\left( \mathbf{P} + \mathbf{Z} \mathbf{W}^{\prime\prime\top}_{m}<br>\right) \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{Z} \in \mathbb{R}^{N_q \times C}$ and $\mathbf{P} \in \mathbb{R}^{N_q \times 2}$ are the query feature matrix and the reference point matrix, respectively, $\Delta\mathbf{P}_m \in \mathbb{R}^{N_q \times K \times 2}$ is the offset matrix for the $m$-th attention head, and $\mathbf{A}_{m} \in \mathbb{R}^{N_q \times K}$ is the attention weight matrix for the $m$-th attention head, and $\mathbf{W}^{\prime\prime}_{m} \in \mathbb{R}^{K \times 2 \times C}$ and $\mathbf{W}^{\prime\prime\prime}_{m} \in \mathbb{R}^{K \times C}$ are the learnable weight matrices for the $m$-th attention head. $\mathbf{Z} \mathbf{W}^{\prime\prime\prime\top}_m \in \mathbb{R}^{N_q \times K}$ is the attention score matrix for the $m$-th attention head, $\mathbf{X}\left( \mathbf{P} + \mathbf{Z} \mathbf{W}^{\prime\prime\top}_{m}<br>\right) \in \mathbb{R}^{N_q \times K \times C}$ is the sampled key feature matrix for the $m$-th attention head, $\text{softmax}\left( \mathbf{Z} \mathbf{W}^{\prime\prime\prime\top}_m \right) \mathbf{X}\left( \mathbf{P} + \mathbf{Z} \mathbf{W}^{\prime\prime\top}_{m}<br>\right) \in \mathbb{R}^{N_q \times C}$.</p>
<h3 id="Deformable-Multi-Head-Attention-Computational-Complexity"><a href="#Deformable-Multi-Head-Attention-Computational-Complexity" class="headerlink" title="Deformable Multi-Head Attention Computational Complexity"></a>Deformable Multi-Head Attention Computational Complexity</h3><p>The computational complexity of the multi-head attention for $N_q$ queries and $K$ sampled keys per query can be derived as follows:</p>
<p>$$<br>\begin{aligned}<br>O\left( M \left( \underbrace{O\left(\min\left( N_q KC + N_q KC + N_q KC_v C , N_q KC + N_k C_v C + N_q K C_v \right)\right)}_{\mathbf{X}\left( \mathbf{P} + \mathbf{Z} \mathbf{W}^{\prime\prime\top}_{m}<br>\right) \mathbf{W}_m^{\prime \top} } + \underbrace{O\left(N_q K C + N_q K\right)}_{\text{softmax}\left( \mathbf{Z} \mathbf{W}^{\prime\prime\prime\top}_m \right)} + O(N_q K C_v) + O(N_q C C_v) \right) \right)<br>&amp;= O\left( N_q K C M + N_q K C + N_q C^2 + \min\left( N_q K C^2, N_k C^2 \right)\right) \\<br>&amp;= O\left( N_q C^2 + \min\left( N_q K C^2, N_k C^2 \right)\right) \\<br>\end{aligned}<br>$$</p>
<p>where $N_k$ is the number of all the key elements and $N_k = HW$. We have a $\min$ operation in the first term because we could either do feature sampling followed by feature transformation or feature transformation followed by feature sampling, whichever is cheaper depending on the values of $N_q$, $N_k = HW$, and $K$.</p>
<p>For the Transformer based computer vision model encoder, because all the attention layers are self-attention, $N_q = N_k = HW$ where $H$ and $W$ are the height and width of the feature map, respectively. The computational complexity of the deformable multi-head attention is $O( N_q C^2 ) = O( H W C^2 )$. Comparing to the $O( H^2 W^2 C )$ computational complexity of the multi-head attention, the computational complexity of the deformable multi-head attention is much cheaper in most cases.</p>
<p>For the decoder, the computational complexity of the deformable multi-head attention is $O( NKC^2 )$ for both the self-attention layers and the cross-attention layers. Comparing to the $O( NC^2 + N^2 C )$ computational complexity of the multi-head attention in the self-attention layers, the computational complexity of the deformable multi-head attention is comparable. Comparing to the $O( HWC^2 + NHWC )$ computational complexity of the multi-head attention in the cross-attention layers, the computational complexity of the deformable multi-head attention is much cheaper in almost all cases.</p>
<h3 id="Deformable-Multi-Head-Attention-VS-Deformable-Convolution"><a href="#Deformable-Multi-Head-Attention-VS-Deformable-Convolution" class="headerlink" title="Deformable Multi-Head Attention VS Deformable Convolution"></a>Deformable Multi-Head Attention VS Deformable Convolution</h3><p>The deformable multi-head attention degenerates to $R \times S$ deformable convolution, where $R$ is the convolution kernel height and $S$ is the convolution kernel width, when $M = RS$, $K = 1$, and $\mathbf{W}_m^{\prime} = \mathbf{I}$ which is an identity matrix.</p>
<h2 id="Multi-Scale-Deformable-Multi-Head-Attention"><a href="#Multi-Scale-Deformable-Multi-Head-Attention" class="headerlink" title="Multi-Scale Deformable Multi-Head Attention"></a>Multi-Scale Deformable Multi-Head Attention</h2><h3 id="Multi-Scale-Deformable-Multi-Head-Attention-Formulation"><a href="#Multi-Scale-Deformable-Multi-Head-Attention-Formulation" class="headerlink" title="Multi-Scale Deformable Multi-Head Attention Formulation"></a>Multi-Scale Deformable Multi-Head Attention Formulation</h3><p>The deformable multi-head attention can be naturally extended to multi-scale deformable multi-head attention to support the multi-scale feature maps from the neural network modules such as feature pyramid network (FPN). It allows the attention to be computed on multiple feature maps with different spatial resolutions.</p>
<img src="/images/blog/2023-12-16-Deformable-Attention/deformable_detr.png" class="box px-0 py-0 ml-auto mr-auto" width="960" title="Multi-Scale Deformable Multi-Head Attention in Deformable DETR" alt="Multi-Scale Deformable Multi-Head Attention in Deformable DETR">
<br>

<p>Given $L$ feature maps $\{\mathbf{X}\}_{l=1}^{L}$ with different spatial resolutions. The $l$-th feature map ${\mathbf{X}}_{l}$ has spatial dimension $H^{l} \times W^{l}$. let $q \in \Omega_q$ indexes a query element with representation feature $\mathbf{z}_q \in \mathbb{R}^{C}$, and $\hat{\mathbf{p}}_q \in [0, 1]^2$ denotes the normalized reference point for $q$, the multi-scale deformable multi-head attention feature is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{MultiScaleDeformableMultiHeadAttention}(\mathbf{z}_q, \hat{\mathbf{p}}_q, \{\mathbf{X}\}_{l=1}^{L})<br>&amp;= \sum_{m = 1}^{M} \mathbf{W}_m \left[ \sum_{l = 1}^{L} \sum_{k = 1}^{K} A_{m,q,l,k} \mathbf{W}_m^{\prime} \mathbf{x}_{l}(\hat{\mathbf{p}}_q + \Delta\mathbf{p}_{m,q,l,k}) \right] \\<br>\end{aligned}<br>$$</p>
<p>where $m$ indexes the attention head, $M$ is the number of attention heads, $\mathbf{W}_m \in \mathbb{R}^{C \times C_{v}}$ and $\mathbf{W}_m^{\prime} \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, $C_{v} = \frac{C}{M}$ by default, $l$ indexes the feature map, $L$ is the number of feature maps, $k$ indexes the sampled key element, $K$ is the number of sampled key elements, $\Delta\mathbf{p}_{m,q,l,k}$ is the offset for the sampled key element $k$ for the $m$-th attention head, and $A_{m,q,l,k} \in \mathbb{R}$ is the attention weight between the query element $q$ and the sampled key element $k$ for the $m$-th attention head and $\sum_{l = 1}^{L} \sum_{k = 1}^{K} A_{m,q,l,k} = 1$.</p>
<h2 id="Deformable-Multi-Head-Attention-V2"><a href="#Deformable-Multi-Head-Attention-V2" class="headerlink" title="Deformable Multi-Head Attention V2"></a>Deformable Multi-Head Attention V2</h2><h3 id="Deformable-Multi-Head-Attention-Drawbacks"><a href="#Deformable-Multi-Head-Attention-Drawbacks" class="headerlink" title="Deformable Multi-Head Attention Drawbacks"></a>Deformable Multi-Head Attention Drawbacks</h3><p>Taking a closer look at the deformable multi-head attention mentioned previously, we can see that the attention weight $A_{m,q,k}$ is computed quite differently from the one in the vanilla multi-head attention. Instead of computing the attention score between the query element $q$ and the key element $k$ via dot product, the attention weight only depends on the query element $q$ for each attention head $m$, which is something a little bit strange in the context of Transformer attention. In fact, the deformable multi-head attention is more closer to convolution than attention.</p>
<p>The deformable multi-head attention also has a drawback of higher memory consumption than the vanilla multi-head attention, especially for the self-attention in the Transformer encoders. In the vanilla multi-head attention, the largest memory consumption comes from the feature map tensor $\mathbf{X} \in \mathbb{R}^{H \times W \times C}$, which is usually very large. In the deformable multi-head attention, because of the new sampled key elements, the memory consumption would be the intermediate sampled key tensor $\mathbf{X} \in \mathbb{R}^{N_q \times K \times C}$. In the Transformer encoders for computer vision models, $N_q = N_k = HW$ is already very large, having $N_q K C = HWKC$ is even more expensive than the vanilla multi-head attention.</p>
<p>When the deformable multi-head attention was first proposed for Transformer based computer vision models, the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.11929">Vision Transformer (ViT)</a> has not been developed yet. Therefore, the deformable multi-head attention was only used in the task head and the feature extraction backbone was still a convolutional neural network (CNN) for Deformable-DETR, producing a feature map of reasonable sized $H$ and $W$, so that the deformable multi-head attention would not be too expensive. In addition, because the deformable multi-head attention was only used in the task head, using small $K$ would not affect the performance too much while reducing the memory consumptions.</p>
<p>The ViT uses the multi-head attention in the Transformer encoders for feature extraction. In this case, because $H$ and $W$ become much larger, the deformable multi-head attention would be too expensive to use. In addition, using small $K$ would affect feature exaction from the backbone. Therefore, using the deformable multi-head attention mentioned above in the Transformer encoders for feature extraction is not a good idea. A new <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.00520">deformable multi-head attention v2</a> was developed to address these issues more specifically for the ViT.</p>
<h3 id="Deformable-Multi-Head-Attention-V2-Formulation"><a href="#Deformable-Multi-Head-Attention-V2-Formulation" class="headerlink" title="Deformable Multi-Head Attention V2 Formulation"></a>Deformable Multi-Head Attention V2 Formulation</h3><p>Developed based on the deformable multi-head attention, the key idea of the deformable multi-head attention v2 is to use a set of global shifted keys shared among all the queries for each attention head. This design is more naturally extended from the vanilla multi-head attention, and it also reduces the memory consumption.</p>
<img src="/images/blog/2023-12-16-Deformable-Attention/deformable_attention_v2.png" class="box px-0 py-0 ml-auto mr-auto" width="960" title="Deformable Multi-Head Attention V2" alt="Deformable Multi-Head Attention V2">
<br>

<p>In the deformable multi-head attention v2, given an input feature map $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$, let $q \in \Omega_q$ indexes a query element with representation feature $\mathbf{z}_q \in \mathbb{R}^{C}$, and $\mathbf{p}_q$ denotes the reference point for $q$, the deformable multi-head attention v2 feature is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{DeformableMultiHeadAttentionV2}(\mathbf{z}_q, \mathbf{p}_q, \mathbf{X}) &amp;= \sum_{m = 1}^{M} \mathbf{W}_m \left[ \sum_{k = 1}^{K} A_{m,q,k} \mathbf{W}_m^{\prime} \mathbf{x}(\mathbf{p}_q + \Delta\mathbf{p}_{m,k}) \right] \\<br>\end{aligned}<br>$$</p>
<p>where $m$ indexes the attention head, $M$ is the number of attention heads, $\mathbf{W}_m \in \mathbb{R}^{C \times C_{v}}$ and $\mathbf{W}_m^{\prime} \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, $C_{v} = \frac{C}{M}$ by default, $k$ indexes the sampled key element, $K$ is the number of sampled key elements, $\Delta\mathbf{p}_{m,k}$ is the offset for the sampled key element $k$ for the $m$-th attention head, and $A_{m,q,k} \in \mathbb{R}$ is the attention weight between the query element $q$ and the sampled key element $k$ for the $m$-th attention head. Notice that unlike the offset $\Delta\mathbf{p}_{m,q,k}$ used in the deformable multi-head attention that is learned for each query element $q$, the offset $\Delta\mathbf{p}_{m,k}$ used in the deformable multi-head attention v2 is shared among all the query elements.</p>
<p>The attention weight $A_{m,q,k}$, which is normalized, is computed almost the same as the one in the vanilla deformable multi-head attention:</p>
<p>$$<br>\begin{aligned}<br>A_{m,q,k} &amp;= \frac{\exp \left( \text{score}(\mathbf{z}_q, \mathbf{x}_k) + b_{m, k} \right)}{\sum_{k^{\prime} = 1}^{K} \exp \left( \text{score}(\mathbf{z}_q, \mathbf{x}_{k^{\prime}}) + b_{m, k} \right)} \\<br>&amp;= \frac{\exp \left( \frac{\left( \mathbf{U}_m \mathbf{z}_q \right)^{\top} \mathbf{V}_m \mathbf{x}_k}{\sqrt{C_v}}  + b_{m, k} \right)}{\sum_{k^{\prime} = 1}^{K} \exp \left( \frac{\left( \mathbf{U}_m \mathbf{z}_q \right)^{\top} \mathbf{V}_m \mathbf{x}_{k^{\prime}}}{\sqrt{C_v}} + b_{m, k} \right)} \\<br>&amp;= \frac{\exp \left( \frac{ \mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_k }{\sqrt{C_v}} + b_{m, k} \right)}{\sum_{k^{\prime} = 1}^{K} \exp \left( \frac{\mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_{k^{\prime}}}{\sqrt{C_v}} + b_{m, k} \right)} \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{U}_m \in \mathbb{R}^{C_{v} \times C}$ and $\mathbf{V}_m \in \mathbb{R}^{C_{v} \times C}$ are the learnable weight matrices for the $m$-th attention head, and $\text{score}(\mathbf{z}_q, \mathbf{x}_k) = \frac{ \mathbf{z}_q^{\top} \mathbf{U}_m^{\top} \mathbf{V}_m \mathbf{x}_k }{\sqrt{C_v}}$ is the attention score between the query element $q$ and the key element $k$ for the $m$-th attention head, and $b_{m, k} \in \mathbb{R}$ is the bias for the $k$-th sampled key element for the $m$-th attention head originally developed for the <a target="_blank" rel="noopener" href="https://arxiv.org/abs/2103.14030">Swin Transformer</a>. The bias $b_{m, k}$ is interpolated from a bias table and its computational cost is negligible comparing to the dominating attention score computation.</p>
<p>The offset $\Delta\mathbf{p}_{m,k}$ depends on all the query elements $q$ for each attention head $m$, and it is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\Delta\mathbf{p}_{m,k} &amp;= f_{m,k} (\mathbf{Z}) \\<br>\end{aligned}<br>$$</p>
<p>where $f_{m,k} (\mathbf{Z})$ is a function of the query feature matrix $\mathbf{Z} \in \mathbb{R}^{N_q \times C}$ for the $m$-th attention head.</p>
<p>The deformable multi-head attention v2 feature for all the queries is computed as follows:</p>
<p>$$<br>\begin{aligned}<br>\text{DeformableMultiHeadAttentionV2}(\mathbf{Z}, \mathbf{P}, \mathbf{X})<br>&amp;= \sum_{m = 1}^{M} \mathbf{A}_{m}  \mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right) \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>&amp;= \sum_{m = 1}^{M} \text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right)^{\top}}{\sqrt{C_v}} + \mathbf{b}_m \right) \mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \left(\mathbf{Z} \right)<br>\right) \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>&amp;= \sum_{m = 1}^{M} \text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}\left( \mathbf{P} + f_{m} \left(\mathbf{Z} \right) \right)^{\top}}{\sqrt{C_v}} + \mathbf{b}_m \right) \mathbf{X}\left( \mathbf{P} + f_{m} \left(\mathbf{Z} \right)<br>\right) \mathbf{W}_m^{\prime \top} \mathbf{W}_m^{\top}  \\<br>\end{aligned}<br>$$</p>
<p>where $\mathbf{Z} \in \mathbb{R}^{N_q \times C}$ and $\mathbf{P} \in \mathbb{R}^{N_q \times 2}$ are the query feature matrix and the reference point matrix, respectively, $\Delta\mathbf{P}_m \in \mathbb{R}^{N_q \times K \times 2}$ is the offset matrix for the $m$-th attention head, and $\mathbf{A}_{m} \in \mathbb{R}^{N_q \times K}$ is the attention weight matrix for the $m$-th attention head, and $\mathbf{W}^{\prime\prime}_{m} \in \mathbb{R}^{K \times 2 \times C}$ and $\mathbf{W}^{\prime\prime\prime}_{m} \in \mathbb{R}^{K \times C}$ are the learnable weight matrices for the $m$-th attention head.</p>
<h3 id="Deformable-Multi-Head-Attention-V2-Computational-Complexity"><a href="#Deformable-Multi-Head-Attention-V2-Computational-Complexity" class="headerlink" title="Deformable Multi-Head Attention V2 Computational Complexity"></a>Deformable Multi-Head Attention V2 Computational Complexity</h3><p>Ignoring the computational complexity of the offset computation $f_{m} \left(\mathbf{Z} \right)$ and the feature map interpolation $\mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right)$ for each head, the computational complexity of the deformable multi-head attention v2 for $N_q$ queries and $K$ sampled shared keys can be derived as follows:</p>
<p>$$<br>\begin{aligned}<br>O\left( M \left( \underbrace{O\left(KC_v C \right)}_{\mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m<br>\right) \mathbf{W}_m^{\prime \top} } + \underbrace{O( K C C_{v} ) + O( N_q C C_{v} ) + O(N_q K C_v) + O(N_q K) + O(K)}_{\text{softmax}\left( \frac{\mathbf{Z} \mathbf{U}_m^{\top}  \mathbf{V}_m \mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right)^{\top}}{\sqrt{C_v}} + \mathbf{b}_m \right) } + O(N_q K C_v) + O(N_q C C_v) \right) \right)<br>&amp;= O\left( M \left( O( K C C_{v} ) + O( N_q C C_{v} ) + O( N_q K C_v ) \right) \right) \\<br>&amp;= O( K C^2 ) + O( N_q C^2 ) + O( N_q K C ) \\<br>&amp;= O( K C^2 + N_q C^2 + N_q K C ) \\<br>\end{aligned}<br>$$</p>
<p>Given the computed offsets $\Delta\mathbf{P}_m$ for each head, the computational complexity of feature map interpolation $\mathbf{X}\left( \mathbf{P} + \Delta\mathbf{P}_m \right)$ is $O(MKC)$. The offset computation $f_{m} \left(\mathbf{Z} \right)$ is usually a small neural network module. Suppose the computational cost of the offset computation is sufficiently small comparing to the other components in the deformable multi-head attention v2, the computational complexity of the deformable multi-head attention v2 for $N_q$ queries and $K$ sampled shared keys is just $O( K C^2 + N_q C^2 + N_q K C + MKC)$.</p>
<p>For the Transformer based computer vision model encoder, because all the attention layers are self-attention, $N_q = N_k = HW$ where $H$ and $W$ are the height and width of the feature map, respectively. The computational complexity of the deformable multi-head attention v2 is $O( K C^2 + N_q C^2 + N_q K C + MKC ) = O( K C^2 + H W C^2 + H W K C + MKC )$. If $K \ll HW$, $M \ll C$, and $M \ll HW$, which is usually the case, the computational complexity of the deformable multi-head attention v2 becomes $O( H W C^2 )$ and it is the same as the one in the deformable multi-head attention and is much cheaper than the one in the vanilla multi-head attention.</p>
<p>Although the deformable multi-head attention v2 was originally developed for the Transformer backbone in the ViT and might have not been used in the Transformer decoder, we could still derive the computational complexity of the deformable multi-head attention v2 in the Transformer decoder. In the self-attention layers, $N_q = N_k = N$, the computational complexity of the deformable multi-head attention v2 is $O( K C^2 + NC^2 + N^2 C + MKC )$. This computation complexity might be comparable to the one used in the vanilla multi-head attention and the one used in the deformable multi-head attention, which is $O( NC^2 + N^2 C )$. In the cross-attention layers, the computational complexity of the deformable multi-head attention v2 is $O( K C^2 + N C^2 + N K C + MKC )$. Because $K &lt; HW$ and $N \ll HW$, this computation complexity is much cheaper than the one used in the vanilla multi-head attention, which is $O( HWC^2 + NHWC )$, and can be comparable to the one used in the deformable multi-head attention, which is $O( NK C^2 )$. Note that the magnitude of $K$ used in the deformable multi-head attention and the deformable multi-head attention v2 can be quite different.</p>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2010.04159">Deformable DETR: Deformable Transformers for End-to-End Object Detection</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2201.00520">Vision Transformer with Deformable Attention</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.05625">PETR: Position Embedding Transformation for Multi-View 3D Object Detection</a></li>
</ul>
</div><div class="article-licensing box"><div class="licensing-title"><p>Deformable Attention</p><p><a href="https://levanliu.github.io/blog/Deformable-Attention/">https://levanliu.github.io/blog/Deformable-Attention/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>Author</h6><p>Lei Mao</p></div></div><div class="level-item is-narrow"><div><h6>Posted on</h6><p>12-16-2023</p></div></div><div class="level-item is-narrow"><div><h6>Updated on</h6><p>12-16-2023</p></div></div><div class="level-item is-narrow"><div><h6>Licensed under</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="https://creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="https://creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="https://creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <div><a class="link-muted" rel="tag" href="/tags/Deep-Learning/">Deep Learning,</a> </div><div><a class="link-muted" rel="tag" href="/tags/Computer-Vision/">Computer Vision,</a> </div><div><a class="link-muted" rel="tag" href="/tags/Transformer/">Transformer,</a> </div><div><a class="link-muted" rel="tag" href="/tags/Attention/">Attention </a> </div></div></div><div class="sharethis-inline-share-buttons"></div><script src="https://platform-api.sharethis.com/js/sharethis.js#property=61b5930d440224001908310c&amp;product=inline-share-buttons" defer></script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered has-text-weight-normal">Like this article? Support the author with</h3><div class="buttons is-centered"><a class="button donate" data-type="paypal" onclick="document.getElementById(&#039;paypal-donate-form&#039;).submit()"><span class="icon is-small"><i class="fab fa-paypal"></i></span><span>Paypal</span></a><form action="https://www.paypal.com/cgi-bin/webscr" method="post" target="_blank" rel="noopener" id="paypal-donate-form"><input type="hidden" name="cmd" value="_donations"><input type="hidden" name="business" value="SSVSLEH4X85LU"><input type="hidden" name="currency_code" value="USD"></form><a class="button donate" href="https://www.buymeacoffee.com/leimao" target="_blank" rel="noopener" data-type="buymeacoffee"><span class="icon is-small"><i class="fas fa-coffee"></i></span><span>Buy me a coffee</span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/essay/%E4%B8%81%E8%83%96%E5%AD%90%E9%87%91%E7%89%8C%E8%AE%B2%E5%B8%88/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">丁胖子金牌讲师</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/life/Coyote-Point-Recreation-Area/"><span class="level-item">Coyote Point Recreation Area 徒步</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card" id="comment-card"><div class="card-content"><h3 class="title is-5">Comments</h3><div class="comment-block"><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a target="_blank" rel="noopener" href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config = function () {
            this.page.url = 'https://levanliu.github.io/blog/Deformable-Attention/';
            this.page.identifier = '/blog/Deformable-Attention/';
        };
        (function() {
            var d = document, s = d.createElement('script');  
            s.src = '//' + 'leimao-github-io' + '.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();</script></div></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" data-type="profile"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img class="avatar is-rounded" src="/img/author_images/Lei-Bio-Medium.jpg" alt="Lei Mao"></figure><p class="title is-size-4 is-block" style="line-height:inherit;margin-bottom:0.30rem;">Lei Mao</p><p class="is-block" style="white-space: pre-line; font-style: italic; margin-bottom: 0.50rem; font-size: 0.8em">Artificial Intelligence
Machine Learning
Computer Science
</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Santa Clara, California</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">Posts</p><a href="/archives"><p class="title">733</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Categories</p><a href="/categories"><p class="title">7</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">Tags</p><a href="/tags"><p class="title">453</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/leimao" target="_blank" rel="noopener"><i class="fab fa-github"></i>  Follow</a> <a class="level-item button is-primary is-rounded" href="https://github.com/sponsors/leimao" target="_blank" rel="noopener"><i class="fas fa-heart"></i>  Sponsor</a></div><div class="level is-mobile is-multiline"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/leimao"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/matchaleimao"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="LinkedIn" href="https://www.linkedin.com/in/lei-mao/"><i class="fab fa-linkedin"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Email" href="mailto:dukeleimao@gmail.com"><i class="fas fa-envelope-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="G.Scholar" href="https://scholar.google.com/citations?user=R2VUf7YAAAAJ"><i class="ai ai-google-scholar-square"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">Catalogue</h3><ul class="menu-list"><li><a class="level is-mobile" href="#Introduction"><span class="level-left"><span class="level-item">1</span><span class="level-item">Introduction</span></span></a></li><li><a class="level is-mobile" href="#Multi-Head-Attention"><span class="level-left"><span class="level-item">2</span><span class="level-item">Multi-Head Attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Multi-Head-Attention-Formulation"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">Multi-Head Attention Formulation</span></span></a></li><li><a class="level is-mobile" href="#Multi-Head-Attention-Computational-Complexity"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">Multi-Head Attention Computational Complexity</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention"><span class="level-left"><span class="level-item">3</span><span class="level-item">Deformable Multi-Head Attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-Formulation"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">Deformable Multi-Head Attention Formulation</span></span></a></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-Computational-Complexity"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">Deformable Multi-Head Attention Computational Complexity</span></span></a></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-VS-Deformable-Convolution"><span class="level-left"><span class="level-item">3.3</span><span class="level-item">Deformable Multi-Head Attention VS Deformable Convolution</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Multi-Scale-Deformable-Multi-Head-Attention"><span class="level-left"><span class="level-item">4</span><span class="level-item">Multi-Scale Deformable Multi-Head Attention</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Multi-Scale-Deformable-Multi-Head-Attention-Formulation"><span class="level-left"><span class="level-item">4.1</span><span class="level-item">Multi-Scale Deformable Multi-Head Attention Formulation</span></span></a></li></ul></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-V2"><span class="level-left"><span class="level-item">5</span><span class="level-item">Deformable Multi-Head Attention V2</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-Drawbacks"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">Deformable Multi-Head Attention Drawbacks</span></span></a></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-V2-Formulation"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">Deformable Multi-Head Attention V2 Formulation</span></span></a></li><li><a class="level is-mobile" href="#Deformable-Multi-Head-Attention-V2-Computational-Complexity"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">Deformable Multi-Head Attention V2 Computational Complexity</span></span></a></li></ul></li><li><a class="level is-mobile" href="#References"><span class="level-left"><span class="level-item">6</span><span class="level-item">References</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="/js/toc.js" defer></script></div><div class="card widget"><div class="g-ads-x"><div class="card-content"><div class="menu"><h3 class="menu-label">Advertisement</h3></div><br><center><script async type="text/javascript" src="//cdn.carbonads.com/carbon.js?serve=CWYDCK3L&amp;placement=leimaogithubio" id="_carbonads_js"></script></center></div></div></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/images/favicon/android-chrome-512x512.png" alt="Lei Mao&#039;s Log Book" height="28"></a><p class="is-size-7"><span>&copy; 2017-2024 Lei Mao</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">Site UV: <span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span></span> <span id="busuanzi_container_site_pv">Site PV: <span id="busuanzi_value_site_pv"><i class="fa fa-spinner fa-spin"></i></span></span></p></div><div class="level-end"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="GitHub" href="https://github.com/leimao"><i class="fab fa-github"></i></a></p></div></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("en");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="/js/column.js"></script><script src="/js/animation.js"></script><a id="back-to-top" title="Back to top" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.10.0/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.8.1/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/javascript" id="MathJax-script" async>MathJax = {
      tex: {
        inlineMath: [['$', '$'], ['\\(', '\\)']]
      },
      svg: {
        fontCache: 'global'
      },
      chtml: {
        matchFontHeight: false
      }
    };</script><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js"></script><!--!--><!--!--><!--!--><script src="/js/main.js" defer></script><script src="/js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="Type something..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"/content.json"}, {"hint":"Type something...","untitled":"(Untitled)","posts":"Posts","pages":"Pages","categories":"Categories","tags":"Tags"});
        });</script></body></html>